---
title: Why Machines Learn
---
_Last updated_: Nov 9, 2024.

These are interesting things mentioned in _Why Machines Learn: The Elegant Maths behind Modern AI_, @ananthaswamyWhyMachinesLearn2024.

**Imprinting**: Konrad Lorenz discovered that ducklings imprint on the first moving thing they see after hatching. More interestingly, they can imprint on _relations_. If upon birth they see two moving red objects, they will later follow two objects of the same color, even if the color is different. More about this in his Nobel lecture [@konradlorenzAnalogySourceKnowledge1973] and biography [@konradlorenzKonradLorenzBiography1973].

**The first artificial neuron**: The paper about the first artificial neuron was a collaboration between McCulloch, a professor in his mid-40s and Pitts, a teenage prodigy who was hanging around a university and was adopted into the McCulloch home. The paper itself [@mccullochLogicalCalculusIdeas1943] is impenetrable, written in a formal math style reminiscient of _Principia Mathematica_. The important conclusion though is that combinations of the artificial neuron can implement any boolean logic.

**Hebbian learning** can be understood as the memorable phrase "neurons that fire together, wire together".

**The Mark I perceptron** was a hardware implementation that could recognize handwritten characters from a 20x20 image. It was a 3-layer neural network, although only one layer had adjustable weights (in hardware, using DC motors to drive potentiometers, essentially volume knobs!). The operator's manual [@Hay1960MarkIP] has all the fascinating details.

William Rowan Hamilton discovered **quaternions** and etched it on a bridge in Dublin. He's also responsible for notions of "scalar" and "vector".

A hyperplane, such as the one learnt by a perceptron, can be uniquely described by a vector that is orthogonal to it. This is in fact the vector of weights, $w$.

The **perceptron learning rule** is simple, but it's remarkable that it always converges if the dataset is linearly separable. The lecture notes [@kilianweinbergerCS457802024] have an accessible proof of this theorem, also reproduced in the book.

When two dialup modems are trying to establish a connection they send a pre-agreed signal that lets them configure (learn) an **adaptive filter** to filter out the noise particular to that line. This is some of the weird sounds we used to hear! The course notes in [@ucberkeleyEECS20NSignalsSystems2011] have more details.

Adaptive filters use the mean-squared error (MSE) as the cost function. When reading about linear regression I've always wondered why we can't just use the absolute difference. One reason to prefer MSE is that it's differentiable everywhere. Another reason mentioned in this book is that the MSE punishes outliers much more than the absolute difference.

The idea of stochastic gradient descent was already invented by the **ADALINE** project at Stanford in the 60s, which tried to solve some of the same problems as the perceptron machine.

Paul Erdős wasn't convinced at first that switching doors in the Monty Hall problem was the right solution. There is hope for all of us!

Bayesian statistics was used by Frederick Mosteller and David Wallace in the 1940s to figure out the authorship of _The Federalist Papers_.

A concise way to remember Principal Component Analysis (PCA): _The eigenvectors of a covariance matrix are the principal components of the original data matrix._

The **support vector machine** (SVM) overcomes the linear separability limitation of the perceptron. It finds an _optimal_ separating hyperplane by projecting the dataset to a much higher dimension and finding a plane there. The algorithm to find this hyperplane works by minimizing a cost function related to the weight vector while simultaneously satisfying a set of constraints, one per data point.

Constrained optimization required by SVMs uses something called the technique of **Lagrange multipliers**. It consists of defining a new function, the Lagrange function, that encodes all the constraints and then finding the extrema of it.

The optimal separating hyperplane of the SVM depends only on the dot produces of a few "support vectors" that anchor the margin, hence the name. However, computing the dot products in higher dimensions can be expensive (this is not very convincing to me – is it still true?). The solution is to use the **kernel trick**.

A _kernel_ is a function such that given two vectors $x_i$ and $x_j$ and a function $\phi(x)$ that transforms each vector to a higher dimension, the kernel $K(x)$ allows one to compute the dot product in the higher dimension while only working with the lower-dimension vectors:

$$
K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)
$$
The kernel trick was suggested by a French scientist Isabelle Guyon, working with Bernard Boser and Vladimir Vapnik. The trick apparently can work even when projecting to an _infinite_ dimensional space, called a Hilbert space. The kernel in that case is called the "radial basis function" (RBF).

An RBF kernel can _always_ find a linearly separable hyperplane in some infinite-dimensional space. This means that SVMs are also **universal function approximators**, just like deep neural networks.

The original SVM paper is [@boserTrainingAlgorithmOptimal1992].

A **spin-glass** is a material that can be thought of as a 3d lattice.

The physicist John Hopfield (Nobel in Physics, 2024) was thinking about the problem of **associative memory**. How is it that when given a fragment of an image or a hint of a smell, we can recall an entire vivid memory?