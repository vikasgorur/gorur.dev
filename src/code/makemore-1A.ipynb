{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddc93d1",
   "metadata": {},
   "source": [
    "# makemore 1 - Exercises\n",
    "\n",
    "This notebook has the exercises from the first video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a8c02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names: 12900\n",
      "Sample names:\n",
      "\n",
      "\tBAAZIGAR\n",
      "\tPARDES\n",
      "\tANURAG MAURYA ACT\n",
      "\tBLACK MARKET\n",
      "\tAKHIYON SE GOLI MAARE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_movie_names() -> list[str]:\n",
    "    movies = pd.read_csv(\"../data/movies.csv\")\n",
    "\n",
    "    def has_special_chars(name: str) -> bool:\n",
    "        AZ = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ \")\n",
    "        return len(set(name) - AZ) > 0\n",
    "\n",
    "    return [\n",
    "        n.upper()\n",
    "        for n in list(movies.query(\"Language == 'hindi'\")[\"Movie Name\"])\n",
    "        if not has_special_chars(n.upper())\n",
    "    ]\n",
    "\n",
    "MOVIES = read_movie_names()\n",
    "print(f\"Number of names: {len(MOVIES)}\")\n",
    "sample_names = \"\\n\\t\".join(MOVIES[5584:5589])\n",
    "print(f\"Sample names:\\n\\n\\t{sample_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0fb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = sorted(list(set(''.join(MOVIES)))) + ['.'] \n",
    "STOI = {s:i for i,s in enumerate(VOCAB)}\n",
    "ITOS = {i:s for s,i in STOI.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b632ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ngrams(corpus: list[str], n: int) -> torch.tensor:\n",
    "    \"Returns the n-grams present in the corpus as a tensor, one row per n-gram\"\n",
    "    # Add start/end tokens to each name\n",
    "    padded_names = [\".\" * (n-1) + name + \".\" for name in corpus]\n",
    "    \n",
    "    # Initialize tensor to store all n-grams\n",
    "    # Each n-gram will be represented as a row of n integers (STOI mappings)\n",
    "    total_ngrams = sum(len(name) - n + 1 for name in padded_names)\n",
    "    result = torch.zeros((total_ngrams, n), dtype=torch.long)\n",
    "    \n",
    "    # Fill the tensor with n-grams\n",
    "    idx = 0\n",
    "    for name in padded_names:\n",
    "        for i in range(len(name) - n + 1):\n",
    "            # Extract the n-gram and convert each character to its STOI index\n",
    "            ngram = name[i:i+n]\n",
    "            for j, char in enumerate(ngram):\n",
    "                result[idx, j] = STOI[char]\n",
    "            idx += 1\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703eacd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trigrams tensor: torch.Size([8, 3])\n",
      "\n",
      "First few trigrams:\n",
      "..C\n",
      ".CA\n",
      "CAT\n",
      "AT.\n",
      "..D\n",
      ".DO\n",
      "DOG\n",
      "OG.\n"
     ]
    }
   ],
   "source": [
    "# Test the ngrams function with a small example\n",
    "test_corpus = [\"CAT\", \"DOG\"]\n",
    "trigrams = ngrams(test_corpus, n=3)\n",
    "print(\"Shape of trigrams tensor:\", trigrams.shape)\n",
    "print(\"\\nFirst few trigrams:\")\n",
    "for i in range(len(trigrams)):\n",
    "    # Convert indices back to characters using ITOS\n",
    "    chars = [ITOS[idx.item()] for idx in trigrams[i]]\n",
    "    print(f\"{''.join(chars)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba15c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_ngram(ngrams: torch.Tensor, n: int, r: float = 1.0) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Train an n-gram model using pre-computed n-grams tensor.\n",
    "    \n",
    "    Args:\n",
    "        ngrams: Tensor of shape (N, n) where N is number of n-grams and each row contains\n",
    "               n STOI indices representing an n-gram\n",
    "        n: The size of n-grams (e.g., 2 for bigrams, 3 for trigrams, etc.)\n",
    "        r: regularization\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (V, V, ..., V) containing normalized n-gram probabilities,\n",
    "        where V is vocabulary size and the tensor has n dimensions\n",
    "    \"\"\"\n",
    "    N_TOKENS = len(VOCAB)\n",
    "    \n",
    "    # Create a sparse tensor of counts using index_put_\n",
    "    # Create a tuple of n dimensions, each of size N_TOKENS\n",
    "    shape = tuple([N_TOKENS] * n)\n",
    "    counts = torch.fill_(torch.zeros(shape), r)\n",
    "    \n",
    "    # Split the ngrams tensor into n columns for index_put_\n",
    "    indices = tuple(ngrams[:, i] for i in range(n))\n",
    "    counts.index_put_(\n",
    "        indices,\n",
    "        torch.ones(len(ngrams)),\n",
    "        accumulate=True\n",
    "    )\n",
    "    \n",
    "    # Normalize the counts into probabilities\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    # Sum over the last dimension for normalization\n",
    "    return counts / (counts.sum(-1, keepdim=True))\n",
    "\n",
    "# Generate trigrams from the corpus\n",
    "trigrams_tensor = ngrams(MOVIES, n=3)\n",
    "model = train_ngram(trigrams_tensor, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3cb2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_loss(model: torch.Tensor, data: torch.Tensor, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate negative log likelihood loss for n-grams using vectorized operations\n",
    "    \n",
    "    Args:\n",
    "        model: Tensor of shape (V, V, ..., V) containing n-gram probabilities\n",
    "        data: Tensor of shape (N, n) containing n-gram indices\n",
    "        n: Size of n-grams (e.g., 2 for bigrams, 3 for trigrams)\n",
    "        \n",
    "    Returns:\n",
    "        Average negative log likelihood across all n-grams\n",
    "    \"\"\"\n",
    "    # Create index tuple for all n dimensions\n",
    "    indices = tuple(data[:, i] for i in range(n))\n",
    "    \n",
    "    # Get probabilities for all n-grams at once using dynamic indexing\n",
    "    probs = model[indices]\n",
    "    \n",
    "    # Calculate log probabilities (adding epsilon for numerical stability)\n",
    "    logprobs = torch.log(probs + 1e-10)\n",
    "    \n",
    "    # Return average negative log likelihood\n",
    "    return -logprobs.mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddda1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram loss = 2.5086231231689453\n",
      "trigram loss = 2.1968438625335693\n"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(MOVIES, 2)\n",
    "trigrams = ngrams(MOVIES, 3)\n",
    "\n",
    "bmodel = train_ngram(bigrams, 2)\n",
    "tmodel = train_ngram(trigrams, 3)\n",
    "\n",
    "print(f\"bigram loss = {ngram_loss(bmodel, bigrams, 2)}\")\n",
    "print(f\"trigram loss = {ngram_loss(tmodel, trigrams, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a9f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram loss = 1.7393230199813843\n"
     ]
    }
   ],
   "source": [
    "qgrams = ngrams(MOVIES, 4)\n",
    "qmodel = train_ngram(qgrams, 4)\n",
    "\n",
    "print(f\"4-gram loss = {ngram_loss(qmodel, qgrams, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71fee308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def split_dataset(\n",
    "        X: torch.tensor,\n",
    "        train: float,\n",
    "        dev: float,\n",
    "        test: float\n",
    "    ) -> Tuple[torch.tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"Split X by row into the 3 datasets\"\n",
    "    assert abs(train + dev + test - 1.0) < 1e-5, \"Proportions must sum to 1\"\n",
    "    n = len(X)\n",
    "    \n",
    "    # Calculate indices for splits\n",
    "    train_idx = int(n * train)\n",
    "    dev_idx = train_idx + int(n * dev)\n",
    "    \n",
    "    # Create random permutation of indices\n",
    "    perm = torch.randperm(n)\n",
    "    \n",
    "    # Split the data using the permuted indices\n",
    "    train_data = X[perm[:train_idx]]\n",
    "    dev_data = X[perm[train_idx:dev_idx]]\n",
    "    test_data = X[perm[dev_idx:]]\n",
    "    \n",
    "    return train_data, dev_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11653e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trigrams, dev_trigrams, test_trigrams = split_dataset(trigrams_tensor, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "53773cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r =  0.01  losses: [train: 2.1891, dev: 2.2900]\n",
      "r =  0.10  losses: [train: 2.1958, dev: 2.2773]\n",
      "r =  0.50  losses: [train: 2.2183, dev: 2.2857]\n",
      "r =  1.00  losses: [train: 2.2405, dev: 2.3014]\n",
      "r =  5.00  losses: [train: 2.3547, dev: 2.3995]\n",
      "r = 10.00  losses: [train: 2.4445, dev: 2.4817]\n"
     ]
    }
   ],
   "source": [
    "for r in [0.01, 0.1, 0.5, 1.0, 5.0, 10.0]:\n",
    "    # Train model on training set only\n",
    "    model = train_ngram(train_trigrams, n=3, r=r)\n",
    "\n",
    "    # Calculate and print losses using the generic ngram_loss function\n",
    "    train_loss = ngram_loss(model, train_trigrams, n=3)\n",
    "    dev_loss = ngram_loss(model, dev_trigrams, n=3)\n",
    "\n",
    "    print(f\"r = {r:5.2f}  losses: [train: {train_loss:.4f}, dev: {dev_loss:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cac4243d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1236484050750732"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train_ngram(test_trigrams, n=3, r=0.10)\n",
    "ngram_loss(model, test_trigrams, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14a43b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test(n: int):\n",
    "    data = ngrams(MOVIES, n)\n",
    "    train_set, dev_set, test_set = split_dataset(data, 0.8, 0.1, 0.1)\n",
    "\n",
    "    model = train_ngram(train_set, n, r=0.1)\n",
    "\n",
    "    train_loss = ngram_loss(model, train_set, n)\n",
    "    dev_loss = ngram_loss(model, dev_set, n)\n",
    "    test_loss = ngram_loss(model, test_set, n)\n",
    "\n",
    "    print(f\"Training set loss: {train_loss:.4f}\")\n",
    "    print(f\"Development set loss: {dev_loss:.4f}\")\n",
    "    print(f\"Test set loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd06b1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set loss: 2.5090\n",
      "Development set loss: 2.5243\n",
      "Test set loss: 2.5005\n"
     ]
    }
   ],
   "source": [
    "train_dev_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ae6c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set loss: 2.2001\n",
      "Development set loss: 2.2449\n",
      "Test set loss: 2.2512\n"
     ]
    }
   ],
   "source": [
    "train_dev_test(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
