---
title: "Makemore 1A: exercises"
date: 2025-08-04
author: Vikas Gorur
toc: true
toc-location: left
toc-title: "Contents"
toc-depth: 6
toc-expand: true
---

This is a follow up to the post [makemore 1](makemore-1.qmd). In this post we'll
do the exercises suggested in the video.

## E01

> Train a trigram language model, i.e. take two characters as an input to
predict the 3rd one. Feel free to use either counting or a neural net. Evaluate
the loss; Did it improve over a bigram model?

We modify the training function to be:

```python
import torch

def train_trigram(corpus: list[str]) -> torch.tensor:
    N_TOKENS = len(VOCAB)
    counts = torch.zeros((N_TOKENS, N_TOKENS, N_TOKENS))
    for name in corpus:
        name = "." + name + "."
        for c1, c2, c_next in zip(name, name[1:], name[2:]):
            counts[STOI[c1], STOI[c2], STOI[c_next]] += 1

    # Normalize the counts into probabilities on each row
    return counts / counts.sum(1, keepdim=True)

model = train_trigram(MOVIES)
```

The model now has the shape `(28, 28, 28)`. The probability distribution for the context `ab`
is given by `model[a, b, :]`.

We compute the average negative log-likelihood loss again:

```python
def nll_loss(names: list[str]) -> float:
    logprobs = 0
    count = 0
    for name in names:
        for c1, c2, c_next in zip(name, name[1:], name[2:]):
            logprobs += math.log(model[STOI[c1], STOI[c2], STOI[c_next]])
            count += 1

    return -logprobs / count

nll_loss(MOVIES)
```

<span class="eval-result">1.8170285469177778</span>

The loss for the bigram model was ~ `2.4913`. It makes sense that the loss is lower for the
trigram model since it takes more information into account in its prediction.

## E02

> Split up the dataset randomly into 80% train set, 10% dev set, 10% test set.
Train the bigram and trigram models only on the training set. Evaluate them on
dev and test splits. What can you see?

## E03

> Use the dev set to tune the strength of smoothing (or regularization) for the
trigram model - i.e. try many possibilities and see which one works best based
on the dev set loss. What patterns can you see in the train and dev set loss as
you tune this strength? Take the best setting of the smoothing and evaluate on
the test set once and at the end. How good of a loss do you achieve?

## E04

> We saw that our 1-hot vectors merely select a row of W, so producing these
vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor
of simply indexing into rows of W?

## E05

> Look up and use `F.cross_entropy` instead. You should achieve the same result.
Can you think of why we'd prefer to use `F.cross_entropy` instead?

## E06

> Meta-exercise! Think of a fun/interesting exercise and complete it.

What is a language model that can achieve a loss of $0$ on the training set?

