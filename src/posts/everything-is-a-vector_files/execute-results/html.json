{
  "hash": "02f37409ef5e873346c895a831d768cb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Everything is a Vector\"\njupyter: python3\n---\n\n\n\n\n_Last updated_: Jan 26, 2025.\n\n---\n\nThere is a simple but powerful idea at the heart of all of Machine Learning. It is considered so obvious that many books and courses treat it as an after-thought. The idea is this:\n\n> All real-world data can be thought of as points in a high-dimensional space.\n\nWhen you understand this it's like realizing that everything in your computer is stored as bits.\n\nIn this post we will explore the power of this idea by applying the same simple ML algorithm to\ndifferent kinds of vectors.\n\n## Nearest neighbors\n\nDescribe the Wifi dataset: https://archive.ics.uci.edu/dataset/422/wireless+indoor+localization\n\nSignal strength of 7 access points, collected in two rooms.\nWe will use a simplified version with only `w5` and `w7`.\n\n\n\n\n```{julia}\nusing CSV, DataFrames, MarkdownTables\n\nwifi = CSV.read(\n    \"data/wifi.tsv\",\n    DataFrame,\n    header=[\"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\", \"w7\", \"room\"]\n)\nwifi = wifi[(wifi.room .== 1) .| (wifi.room .== 2), [:w5, :w7, :room]]\nmarkdown_table(wifi[rand(1:size(wifi, 1), 5), :])\n```\n\n\n\n\nA **vector** in machine learning is a point in $n$-dimensional space.\n\nDistance between two points:\n\n\n\n\n```{julia}\ndistance(v1, v2) = sqrt(sum((v1 - v2).^2))\ndistance([0, 0], [3, 4])\n```\n\n\n\n\n`sortperm` - return indexes of the form [smallest element, next smallest element, ...]\n\n\n\n```{julia}\nprint(sortperm([4, 3, 1, 2]))\n```\n\n\n\n\nDescribe the knn algorithm.\n\n- points with labels\n- knn\n\n\n\n\n```{julia}\n\nstruct Point\n    xn::Vector{Float64}\n    label::String\nend\n\nfunction knn(X::Array{Point}, v::Vector{Float64}, k::Int)\n    ds = [distance(x.xn, v) for x in X]\n    return X[sortperm(ds)[1:k]]\nend\n```\n\n\n\n\nExample of knn working in 2 dimensions\n\n\n\n\n```{julia}\nknn([\n    Point([0.0, 0.0], \"zero\"),\n    Point([1.0, 1.0], \"one\"),\n    Point([2.0, 2.0], \"two\"),\n    Point([3.0, 3.0], \"three\"),\n    Point([4.0, 4.0], \"four\"),\n    Point([5.0, 5.0], \"five\")\n], [3.0, 3.0], 3)\n```\n\n\n\n\nTurn the dataset into `Point`s.\n\n\n\n\n```{julia}\n#| output: false\n# Iterate across each row of wifi and create a Point for it\nX = [\n    Point(collect(row[[:w5, :w7]]), string(row[:room]))\n    for row in eachrow(wifi)\n]\n```\n\n\n\n\n- Explain the prediction algorithm.\n- Describe train/test split.\n- Result of running knn on the wifi dataset.\n\n\n\n\n```{julia}\nusing MLUtils\n\nX_test, X_train = splitobs(X, at=0.15)\nX_train = collect(X_train)\nX_test = collect(X_test)\n\nsize(X_train)\n\"Return the element that occurs most frequently in an array\"\nfunction majority(items::Vector{T})::T where T\n    c = Dict{T, Int}()\n    for it in items\n        if !haskey(c, it)\n            c[it] = 1\n        else\n            c[it] += 1\n        end\n    end\n    return sort(collect(c), by=x->x[2], rev=true)[1][1]\nend\n\n# Compute the accuracy score\ntotal = 0\ncorrect = 0\n\nfor p in X_test\n    neighbors = knn(X_train, p.xn, 7)\n    label = majority([x.label for x in neighbors])\n    if label == p.label\n        correct += 1\n    end\n    total += 1\nend\n\nprintln(\"Accuracy: $(correct / total * 100.0)%\")\n```\n\n\n\n\nDraw the scatter plot\n\n\n\n\n```{julia}\nusing PlotlyJS\n\nplot(scatter(\n    x = [p.xn[1] for p in X_train],\n    y = [p.xn[2] for p in X_train],\n    mode = \"markers\",\n))\n```\n\n\n\n\n## Words as vectors\n\nEncode a recipe as a vector by doing a one-hot encoding of each of the words\nin the recipe.\n\nUse knn to predict the cuisine of a new recipe.\n\n## Words as vectors (better)\n\nUse text embeddings to turn a recipe into a vector.\nHow to combine vectors for individual words into vector for the whole recipe?\n\n## closing thoughts\n\nwhat else are vectors?\n- images\n- sound\n\n## Further reading\n\nWord2vec paper, won the \"test of time\" award\n\n*Distributed Representations of Words and Phrases and their Compositionality* https://arxiv.org/abs/1310.4546\n\nThe illustrated Word2vec https://jalammar.github.io/illustrated-word2vec/\n\n",
    "supporting": [
      "everything-is-a-vector_files"
    ],
    "filters": [],
    "includes": {}
  }
}