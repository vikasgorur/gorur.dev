---
title: Path to LLMs
bibliography: ../zotero.bib
---

*Last updated*: Oct 18, 2024.

---

This post is my attempt to draw the shortest path from knowing a little bit of ML
to understanding state of the art language models. It includes both milestone
papers and the best resources I've found to understand a concept. I also like knowing 
the history of things so there will be a bunch of papers that might really
only be of historical interest.

This is a _personal_ path, with the goal of being a reasonably
good practitioner of ML, not a researcher. Finally, "path" is a misnomer.
It's more like a garden to get lost in.

## Math background

There is no end to the amount of math one *could* learn before studying ML, and
usually the more I learn the more it seems to help. However, I've also found that
it's ok to "lazy-load" the required math once you've acquired a decent intuition
in each of the major areas. This section therefore is just going to be a list of
the areas of math that can be helpful and the best resources I've found for learning
them.

Ever since I discovered computers my identity has been "programmer". The book by
@jeremykunProgrammersIntroductionMathematics2021 changed my *relationship* to math and gave me the confidence to 
read the ML textbooks and papers. It helped me reconnect with my teenage self that found
math playful and was excited by it rather than scared by notation.
**This is a life-changing book.**

### Probability

Probability is the foundation for all of ML, statistics, and science. It's also _way more_
complicated than our brief encounter with it in high school or college makes us believe. I'm
always on the look out for books and articles that help in developing a good intuition
for probability.

The textbook by [@hammingArtProbabilityScientists1991] is one of the best introductions. It is rigorous enough
for us engineers but more importantly has long passages that explain the intuition behind
ideas.

### Information Theory

Information seems like the most natural concept to try to understand ML and stats.
Many of the questions of interest can be posed as information theory questions:
"what has a model learnt?", or "what did this experiment tell us?",
"how much can a model of a certain size learn?"

[@coverElementsInformationTheory2005] and [@mackayInformationTheoryInference2003] are two useful textbooks.

### Linear Algebra

Linear Algebra has the worst branding in all of math. It's more exciting
to think of the subject as "thinking in high-dimensional spaces". Everything in ML
deals with vectors with impossibly high dimensions (for example, each token in GPT3
is represented as a vector in a ~50,000 dimension space).

The video series "Essence of Linear Algebra" by [@3blue1brownEssenceLinearAlgebra2016] was the
first time linear algebra made any intuitive sense to me.

### Calculus

ML papers are full of complicated equations with symbols from multivariate and
matrix calculus. This might give the impression that one needs a full undergrad course
in these topics before making any progress, but I don't buy it. I think one can get by
for a long time with just the intuition of the concept of a derivative (gradient) for
complicated functions and the chain rule for computing them.

## Optimization in ML

The goal of all ML training is to find an acceptably low value of the loss function. This is the
part of ML that I find it the easiest to treat as a black box.

[@bottouOptimizationMethodsLargeScale2016] is a great overview of the various optimization methods
used in ML.

## Automatic Differentiation

AD is the key to training large neural networks. AD libraries automatically
figure out the gradient of the loss function as long as the computation of the loss
function is expressed in a form that the library expects. For example, in PyTorch
the computation is expressed as tensor operations.

[@baydinAutomaticDifferentiationMachine2015] is a great survey of the various AD methods. For ML training
we care about "reverse mode". [@paszkePyTorchImperativeStyle2019] describes PyTorch, the most widely used library for deep learning
in production.

## What are neural networks?

The first neural network was the perceptron [see @nilssonQuestArtificialIntelligence2010, section 4.2.1], a single-layer network built to identify
objects in 20x20 pixel images. I find it fascinating to note that most of the early work
on neural networks was done by people trying to understand human cognition
by building a model of computation different from the familiar digital (von Neumann) computer.
From that perspective, current LLMs running on GPUs are just one physical realization of the
model of computation.

The key algorithm for training neural networks is backpropagation. This algorithm has apparently
been invented independently many times. [@rumelhartLearningRepresentationsBackpropagating1986] is one of the widely cited descriptions
of it.

[@lecunBackpropagationAppliedHandwritten1989] is one of the first examples of using neural networks and back propagation
to solve the recognizably modern problem of handwriting recognition. An interesting companion piece is the blog post [@karpathyDeepNeuralNets2022] that re-implements the network described in the original paper and illustrates the massive difference in training time made possible by modern hardware.

Another milestone in the deep learning revolution is AlexNet [@krizhevskyImageNetClassificationDeep2012] where
a deep learning model beat all other previous computer vision models on image recognition
by a significant margin. This paper also illustrates the coming together of three factors that make
deep learning practical and are true to this day: (1) massive datasets (2) GPUs for efficient matrix
computations (3) libraries to do automatic differentiation easily.

## What is language modeling?

The task of language modeling is to learn a probability distribution about a corpus. The
distribution is the conditional probability of the next token given a sequence of previous tokens. A
short introduction to language modeling is in [@hangliLanguageModelsPresent2022].

The roots of this go back to Markov analyzing Pushkin's poetry to settle a debate about free will(!),
described in the article by [@hayesFirstLinksMarkov2013].

The classic [@shannonMathematicalTheoryCommunication1948] paper that invented information theory also considers language modeling, as does his subsequent paper [@shannonPredictionEntropyPrinted1951]. In the second paper he describes an experiment to figure out the entropy of English language by giving humans (his wife and another couple) the task of predicting the next word of a short sentence, essentially treating them like modern LLMs!

The Shannon living-room experiments story is related in this entertaining profile: [@horganClaudeShannonProfile1992]

## How is language modeling done with neural networks?

[@bengioNeuralProbabilisticLanguage2003] introduced the ideas of using a neural network to model language as well
as the idea of a "distributed representation", also known as word embeddings. The goal of embedding
is to turn words and phrases into vectors in a high-dimensional space.

A big step forward in embeddings was Google's word2vec paper [@mikolovDistributedRepresentationsWords2013], which contained the famous example
`vec("King") - vec("Man") + vec("Woman") ~= vec("Queen")`. Embeddings just on their own are an incredibly useful tool in building products because they capture a general notion of semantic "distance" between words, sentences, or entire documents.

Recurrent Neural Nets (RNNs) were one solution to the problem of capturing the *sequential* nature of language. The historical roots of this approach are in the cognitive science paper [@elmanFindingStructureTime1990]. The blog post [@andrejkarpathyUnreasonableEffectivenessRecurrent2015] illustrates the "unreasonable effectiveness" of RNNs.

The playlist "Neural Networks: Zero to Hero" [@karpathyNeuralNetworksZero2022] is a step-by-step walkthrough to building something like GPT-2 starting from nothing but knowledge of Python. This entire post is in a sense is all the supplementary reading I'm doing to finish understanding all the videos in this playlist.

## Large Language Models

Everything in this section is just the starting point for deeper rabbit holes.

"Attention is all you need" [@vaswaniAttentionAllYou2017] contains the core DNA of all current LLMs. Everything I described in this post above is my attempt to get to a full understanding of this landmark paper.

"State of GPT" [@andrejkarpathyStateGPT2023] is the best 1-hour introduction to the architecture, training and capabilities of LLMs. This talk is accessible to any working programmer, it doesn't need any previous knowledge of LLMs or neural networks.

[@3blue1brownNeuralNetworks2024] is a great series of videos on neural networks and deep learning,
with recent videos focusing on LLMs.

The papers on open source LLMs have a wealth of detail on the training data and methodology.
See LLAMA2 [@touvronLlama2Open2023], Mistral 7B [@jiangMistral7B2023].

The effectiveness of neural networks is extremely dependent on the quantity and quality of the training data. This fact is apparently discovered again and again so often that it has a name: "the bitter lesson" [@richsuttonBitterLesson2019]. An intriguing related fact about LLMs is the existence of "scaling laws" that describe the optimal model size and number of training tokens for a given compute budget [@hoffmannTrainingComputeOptimalLarge2022].

The wide applicability of LLMs is a result of their ability to learn to perform tasks with just
a handful of examples ("few-shot learning"). This discovery is related in the GPT2 [@brownLanguageModelsAre2020] and GPT3 papers [@kojimaLargeLanguageModels2022].

Training LLMs is an incredibly complicated systems engineering problem. This blog post by the lead of PyTorch [@chintalaHowTrainModel2024] and the infrastructure section in the LLama3 paper [@dubeyLlama3Herd2024] provide insight into what it takes.

[ *to be continued* … ]
