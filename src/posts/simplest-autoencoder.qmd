---
title: The Simplest Autoencoder
jupyter: python3
---
The autoencoder.

```{python}
import marimo as mo
import numpy as np
import torch
from torch import nn
from torchviz import make_dot
```

Define the inputs.

```{python}
INPUTS = torch.tensor([
     1, 0, 0, 0, 0, 0, 0, 0,
     0, 1, 0, 0, 0, 0, 0, 0,
	 0, 0, 1, 0, 0, 0, 0, 0,
	 0, 0, 0, 1, 0, 0, 0, 0,
	 0, 0, 0, 0, 1, 0, 0, 0,
     0, 0, 0, 0, 0, 1, 0, 0,
	 0, 0, 0, 0, 0, 0, 1, 0,
	 0, 0, 0, 0, 0, 0, 0, 1],
	 dtype=torch.float32).reshape(8, 8)
```

Define the network

```{python}
from collections import OrderedDict

class AutoEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.stack = nn.Sequential(OrderedDict([
            ('input', nn.Linear(8, 3)),
            ('sigmoid', nn.Sigmoid()),
            ('hidden', nn.Linear(3, 8)),
            ('sigmoid', nn.Sigmoid())
        ]))

    def forward(self, x):
        return self.stack(x)
```

Visualize the network

```{python}
from torchview import draw_graph

def visualize(model: nn.Module):
    g = draw_graph(model, input_size=(8,)).visual_graph
    g.graph_attr["rankdir"] = 'LR'
    return g.render()

#visualize(AutoEncoder())
```

Train it

```{python}
def train(epochs: int):
    model = AutoEncoder()
    cross_entropy_loss = nn.CrossEntropyLoss()
    sgd = torch.optim.SGD(model.parameters(), lr=0.001)

    prev_loss = 1e9
    loss = 0
    iters = []
    losses = []
    for i in range(epochs):
        x = INPUTS
        yhat = model(x)
        prev_loss = loss
        loss = cross_entropy_loss(yhat, x)

        loss.backward()
        sgd.step()

        if abs(loss - prev_loss) < 1e-6:
            print(f"i = {i}, loss: {loss:>7f}")
            break

        if i % 100 == 0:
            print(f"i = {i}, loss: {loss:>7f}")

        iters.append(i)
        losses.append(loss.item())

    return model, iters, losses
```

Visualize the loss

```{python}
import matplotlib.pyplot as plt

torch.manual_seed(42)
net, iters, losses = train(10000)
plt.plot(iters, losses)
```

Outputs

```{python}
for i in range(8):
    print(np.argmax(net(INPUTS[i]).detach().numpy()))
```

Things I learned while training this network:

- MSE loss doesn't work.
- Gradient descent with one example at a time doesn't work very reliably.

The point of the autoencoder is to learn two functions, the encoder and decoder.
Each of the weight matrices can be thought of as a function.

Print out the 3-element representations for each input.
Try reducing the floats to 16 bit or lower?

Can a single layer with sigmoid learn this?

Clojure

```clojure
(defn dot
  "Dot product of two vectors"
  [a b]
  (reduce + 0.0 (map * a b)))

(dot [1 2 3] [4 5 6])
;;=> 32.0
```
