@misc{3blue1brownEssenceLinearAlgebra2016,
  title = {Essence of {{Linear Algebra}}},
  shorttitle = {3blue1brown - Linear Algebra},
  author = {{3blue1brown}},
  year = {2016},
  month = aug
}

@misc{3blue1brownNeuralNetworks2024,
  title = {Neural {{Networks}}},
  author = {{3blue1brown}},
  year = {2024}
}

@book{ananthaswamyWhyMachinesLearn2024,
  title = {Why Machines Learn: The Elegant Math behind Modern {{AI}}},
  shorttitle = {Why Machines Learn},
  author = {Ananthaswamy, Anil},
  year = {2024},
  publisher = {Dutton},
  address = {New York},
  abstract = {"A rich, narrative explanation of the mathematics that has brought us machine learning and the ongoing explosion of artificial intelligence"--},
  isbn = {978-0-593-18574-2},
  lccn = {Q325.5 .A56 2024}
}

@misc{andrejkarpathyStateGPT2023,
  title = {State of {{GPT}}},
  author = {{Andrej Karpathy}},
  year = {2023},
  month = may
}

@misc{andrejkarpathyUnreasonableEffectivenessRecurrent2015,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  author = {{Andrej Karpathy}},
  year = {2015},
  month = may
}

@misc{bartosFairCoinsTend2023,
  title = {Fair Coins Tend to Land on the Same Side They Started: {{Evidence}} from 350,757 Flips},
  shorttitle = {Fair Coins Tend to Land on the Same Side They Started},
  author = {Barto{\v s}, Franti{\v s}ek and Sarafoglou, Alexandra and Godmann, Henrik R. and Sahrani, Amir and Leunk, David Klein and Gui, Pierre Y. and Voss, David and Ullah, Kaleem and Zoubek, Malte J. and Nippold, Franziska and Aust, Frederik and Vieira, Felipe F. and Islam, Chris-Gabriel and Zoubek, Anton J. and Shabani, Sara and Petter, Jonas and Roos, Ingeborg B. and Finnemann, Adam and Lob, Aaron B. and Hoffstadt, Madlen F. and Nak, Jason and {de Ron}, Jill and Derks, Koen and Huth, Karoline and Terpstra, Sjoerd and Bastelica, Thomas and Matetovici, Magda and Ott, Vincent L. and Zetea, Andreea S. and Karnbach, Katharina and Donzallaz, Michelle C. and John, Arne and Moore, Roy M. and Assion, Franziska and {van Bork}, Riet and Leidinger, Theresa E. and Zhao, Xiaochang and Motaghi, Adrian Karami and Pan, Ting and Armstrong, Hannah and Peng, Tianqi and Bialas, Mara and Pang, Joyce Y. -C. and Fu, Bohan and Yang, Shujun and Lin, Xiaoyi and Sleiffer, Dana and Bognar, Miklos and Aczel, Balazs and Wagenmakers, Eric-Jan},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.04153},
  urldate = {2024-11-19},
  abstract = {Many people have flipped coins but few have stopped to ponder the statistical and physical intricacies of the process. In a preregistered study we collected \$350\{,\}757\$ coin flips to test the counterintuitive prediction from a physics model of human coin tossing developed by Diaconis, Holmes, and Montgomery (DHM; 2007). The model asserts that when people flip an ordinary coin, it tends to land on the same side it started -- DHM estimated the probability of a same-side outcome to be about 51\%. Our data lend strong support to this precise prediction: the coins landed on the same side more often than not, \${\textbackslash}text\{Pr\}({\textbackslash}text\{same side\}) = 0.508\$, 95\% credible interval (CI) [\$0.506\$, \$0.509\$], \${\textbackslash}text\{BF\}\_\{{\textbackslash}text\{same-side bias\}\} = 2359\$. Furthermore, the data revealed considerable between-people variation in the degree of this same-side bias. Our data also confirmed the generic prediction that when people flip an ordinary coin -- with the initial side-up randomly determined -- it is equally likely to land heads or tails: \${\textbackslash}text\{Pr\}({\textbackslash}text\{heads\}) = 0.500\$, 95\% CI [\$0.498\$, \$0.502\$], \${\textbackslash}text\{BF\}\_\{{\textbackslash}text\{heads-tails bias\}\} = 0.182\$. Furthermore, this lack of heads-tails bias does not appear to vary across coins. Additional exploratory analyses revealed that the within-people same-side bias decreased as more coins were flipped, an effect that is consistent with the possibility that practice makes people flip coins in a less wobbly fashion. Our data therefore provide strong evidence that when some (but not all) people flip a fair coin, it tends to land on the same side it started. Our data provide compelling statistical support for the DHM physics model of coin tossing.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Data Analysis Statistics and Probability (physics.data-an),FOS: Computer and information sciences,FOS: Mathematics,FOS: Physical sciences,History and Overview (math.HO),Other Statistics (stat.OT)},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/350757Flips.pdf}
}

@article{baydinAutomaticDifferentiationMachine2015,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  shorttitle = {{{AD}} Survey},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2015},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1502.05767},
  urldate = {2024-10-11},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  year = {2003},
  month = mar,
  journal = {J. Mach. Learn. Res.},
  volume = {3},
  number = {null},
  pages = {1137--1155},
  publisher = {JMLR.org},
  issn = {1532-4435},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.}
}

@book{blitzsteinIntroductionProbability2019,
  title = {Introduction to Probability},
  author = {Blitzstein, Joseph K. and Hwang, Jessica},
  year = {2019},
  edition = {Second edition},
  publisher = {CRC Press},
  address = {Boca Raton},
  abstract = {"Undergraduate probability book that assumes one-semester of calculus. One key is the emphasis on "stories" for the probability distributions (which I mean in both an intuitive and technical sense): there are a dozen or so key distributions (Normal, Binomial, Poisson, etc.) that are incredibly widely-used in statistics, but a lot of books just write down formulas for them without explaining clearly why these particular distributions are so important, or how they are all connected. Each of these distributions has a "story" (a natural application where it arises), and thinking about stories makes the distributions easier to remember, understand, and work with"--},
  isbn = {978-0-429-76673-2 978-0-429-42835-7},
  lccn = {QA273},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/Introduction to Probability by Joseph K. Blitzstein, Jessica Hwang (z-lib.org).pdf}
}

@article{blitzsteinSolutionsIntroductionProbability,
  title = {Solutions - {{Introduction}} to {{Probability}}},
  author = {Blitzstein, Joseph K and Hwang, Jessica},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/selected_solutions_blitzstein_hwang_probability_01.pdf}
}

@article{blitzsteinStat110Strategic,
  title = {Stat 110 {{Strategic Practice}} 3},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_3.pdf}
}

@article{blitzsteinStat110Strategica,
  title = {Stat 110 {{Strategic Practice}} 11},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_11.pdf}
}

@article{blitzsteinStat110Strategicb,
  title = {Stat 110 {{Strategic Practice}} 10},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_10.pdf}
}

@article{blitzsteinStat110Strategicc,
  title = {Stat 110 {{Strategic Practice}} 5},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_5.pdf}
}

@article{blitzsteinStat110Strategicd,
  title = {Stat 110 {{Strategic Practice}} 6},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_6.pdf}
}

@article{blitzsteinStat110Strategice,
  title = {Stat 110 {{Strategic Practice}} 7},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_7.pdf}
}

@article{blitzsteinStat110Strategicf,
  title = {Stat 110 {{Strategic Practice}} 8},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_8.pdf}
}

@article{blitzsteinStat110Strategicg,
  title = {Stat 110 {{Strategic Practice}} 9},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_9.pdf}
}

@article{blitzsteinStat110Strategich,
  title = {Stat 110 {{Strategic Practice}} 4},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_4.pdf}
}

@article{blitzsteinStat110Strategici,
  title = {Stat 110 {{Strategic Practice}} 2},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_2.pdf}
}

@article{blitzsteinStat110Strategicj,
  title = {Stat 110 {{Strategic Practice}} 1},
  author = {Blitzstein, Joe},
  journal = {A man},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_1.pdf}
}

@inproceedings{boserTrainingAlgorithmOptimal1992,
  title = {A Training Algorithm for Optimal Margin Classifiers},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  year = {1992},
  month = jul,
  pages = {144--152},
  publisher = {ACM},
  address = {Pittsburgh Pennsylvania USA},
  doi = {10.1145/130385.130401},
  urldate = {2024-11-09},
  isbn = {978-0-89791-497-0},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/A Training Algorithm for Optimal Margin Classifiers - Boser 1992.pdf}
}

@misc{bottouOptimizationMethodsLargeScale2016,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1606.04838},
  urldate = {2024-10-18},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{breimanReflectionsRefereeingPapers2018,
  title = {Reflections {{After Refereeing Papers}} for {{NIPS}}},
  booktitle = {The {{Mathematics}} of {{Generalization}}},
  author = {Breiman, Leo},
  year = {2018},
  month = mar,
  edition = {1},
  pages = {11--15},
  publisher = {CRC Press},
  doi = {10.1201/9780429492525-2},
  urldate = {2024-11-27},
  collaborator = {Wolpert, David. H},
  isbn = {978-0-429-49252-5},
  langid = {english},
  file = {/Users/vikas/Zotero/storage/H3DPWCQ7/Breiman - 2018 - Reflections After Refereeing Papers for NIPS.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2005.14165},
  urldate = {2024-10-12},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{burkemanFreeYourMind2024,
  title = {Free {{Your Mind}}. {{The Election Will Follow}}.},
  author = {Burkeman, Oliver},
  year = {2024},
  month = nov,
  journal = {The New York Times},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Opinion _ Free Your Mind. The Election Will Follow. - The New York Times.pdf}
}

@misc{chintalaHowTrainModel2024,
  title = {How to Train a Model on 10k {{H100 GPUs}}?},
  author = {Chintala, Soumith},
  year = {2024},
  month = oct
}

@book{coverElementsInformationTheory2005,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2005},
  month = sep,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/047174882X},
  urldate = {2024-10-18},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-0-471-24195-9 978-0-471-74882-3},
  langid = {english}
}

@article{davida.pattersonLifeLessonsFirst2024,
  title = {Life {{Lessons}} from the {{First Half-Century}} of {{My Career}}},
  author = {{David A. Patterson}},
  year = {2024},
  month = oct,
  journal = {Communications of the ACM},
  pages = {3637905},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3637905},
  urldate = {2024-10-12},
  langid = {english}
}

@misc{dubeyLlama3Herd2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and {van der Linde}, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and {Rantala-Yeary}, Lauren and {van der Maaten}, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and {de Oliveira}, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzm{\'a}n, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, V{\'i}tor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and {Wang} and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2407.21783},
  urldate = {2024-10-18},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  year = {1990},
  month = mar,
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {0364-0213, 1551-6709},
  doi = {10.1207/s15516709cog1402_1},
  urldate = {2024-10-18},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english}
}

@book{hammingArtProbabilityScientists1991,
  title = {The {{Art}} of {{Probability}}: {{For Scientists}} and {{Engineers}}},
  shorttitle = {The {{Art}} of {{Probability}}},
  author = {Hamming, Richard W.},
  year = {1991},
  edition = {1},
  publisher = {CRC Press},
  doi = {10.1201/9780429492952},
  urldate = {2024-10-12},
  isbn = {978-0-429-49295-2},
  langid = {english}
}

@article{hangliLanguageModelsPresent2022,
  title = {Language Models: Past, Present, and Future},
  shorttitle = {History of Language Modeling},
  author = {{Hang Li}},
  year = {2022},
  month = jul,
  journal = {Communications of the ACM},
  volume = {65},
  number = {7},
  pages = {56--63},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3490443},
  urldate = {2024-10-07},
  abstract = {A language modeling overview, highlighting basic concepts, intuitive explanations, technical achievements, and fundamental challenges.},
  langid = {english},
  keywords = {note-published},
  file = {/Users/vikasprasad/Downloads/Hang Li 2022 - Language Models Past Present.pdf}
}

@techreport{Hay1960MarkIP,
  title = {Mark {{I}} Perceptron Operators' Manual},
  author = {Hay, John C. and Lynch, B. and Smith, David Russell Bedford},
  year = {1960},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Mark I Perceptron operators manual.pdf}
}

@article{hayesFirstLinksMarkov2013,
  title = {First {{Links}} in the {{Markov Chain}}},
  author = {Hayes, Brian},
  year = {2013},
  journal = {American Scientist},
  volume = {101},
  number = {2}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2203.15556},
  urldate = {2024-10-18},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J J},
  year = {1982},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.79.8.2554},
  urldate = {2024-11-20},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/hopfield-1982-neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.pdf}
}

@article{horganClaudeShannonProfile1992,
  title = {Claude {{E}}. {{Shannon}} [{{Profile}}]},
  author = {Horgan, J.},
  year = {1992},
  month = apr,
  journal = {IEEE Spectrum},
  volume = {29},
  number = {4},
  pages = {72--75},
  issn = {0018-9235, 1939-9340},
  doi = {10.1109/MSPEC.1992.672257},
  urldate = {2024-10-18},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@book{hwuProgrammingMassivelyParallel2023,
  title = {Programming Massively Parallel Processors: A Hands-on Approach},
  shorttitle = {Programming Massively Parallel Processors},
  author = {Hwu, Wen-mei W. and Kirk, David and El Hajj, Izzat},
  year = {2023},
  edition = {Fourth edition},
  publisher = {Morgan Kaufmann, Elsevier},
  address = {Cambridge, MA},
  doi = {10.1016/C2020-0-02969-5},
  abstract = {Programming Massively Parallel Processors: A Hands-on Approach shows both students and professionals alike the basic concepts of parallel programming and GPU architecture. Concise, intuitive, and practical, it is based on years of road-testing in the authors' own parallel computing courses. Various techniques for constructing and optimizing parallel programs are explored in detail, while case studies demonstrate the development process, which begins with computational thinking and ends with effective and efficient parallel programs. The new edition includes updated coverage of CUDA, including the newer libraries such as CuDNN. New chapters on frequently used parallel patterns have been added, and case studies have been updated to reflect current industry practices},
  isbn = {978-0-323-91231-0 978-0-323-98463-8},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Programming Massively Parallel Processors 4th ed.pdf}
}

@book{jeremykunProgrammersIntroductionMathematics2021,
  title = {A {{Programmer}}'s {{Introduction}} to {{Mathematics}}},
  author = {{Jeremy Kun}},
  year = {2021}
}

@misc{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.06825},
  urldate = {2024-10-18},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{johnc.baezWhatEntropy2024,
  title = {What Is {{Entropy}}?},
  author = {{John C. Baez}},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2409.09232},
  urldate = {2024-10-12},
  abstract = {This short book is an elementary course on entropy, leading up to a calculation of the entropy of hydrogen gas at standard temperature and pressure. Topics covered include information, Shannon entropy and Gibbs entropy, the principle of maximum entropy, the Boltzmann distribution, temperature and coolness, the relation between entropy, expected energy and temperature, the equipartition theorem, the partition function, the relation between expected energy, free energy and entropy, the entropy of a classical harmonic oscillator, the entropy of a classical particle in a box, and the entropy of a classical ideal gas.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{jordaRateReturnEverything2019,
  title = {The {{Rate}} of {{Return}} on {{Everything}}, 1870--2015*},
  author = {Jord{\`a}, {\`O}scar and Knoll, Katharina and Kuvshinov, Dmitry and Schularick, Moritz and Taylor, Alan M},
  year = {2019},
  month = aug,
  journal = {The Quarterly Journal of Economics},
  volume = {134},
  number = {3},
  pages = {1225--1298},
  issn = {0033-5533, 1531-4650},
  doi = {10.1093/qje/qjz012},
  urldate = {2024-11-03},
  abstract = {Abstract             What is the aggregate real rate of return in the economy? Is it higher than the growth rate of the economy and, if so, by how much? Is there a tendency for returns to fall in the long run? Which particular assets have the highest long-run returns? We answer these questions on the basis of a new and comprehensive data set for all major asset classes, including housing. The annual data on total returns for equity, housing, bonds, and bills cover 16 advanced economies from 1870 to 2015, and our new evidence reveals many new findings and puzzles.},
  copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Jorda - The Rate of Return on Everything.pdf}
}

@misc{karpathyDeepNeuralNets2022,
  title = {Deep {{Neural Nets}}: 33 Years Ago and 33 Years from Now},
  author = {Karpathy, Andrej},
  year = {2022},
  month = mar
}

@misc{karpathyMakemore2022,
  title = {Makemore},
  author = {Karpathy, Andrej},
  year = {2022},
  month = jun,
  copyright = {MIT License}
}

@misc{karpathyNeuralNetworksZero2022,
  title = {Neural {{Networks}}: {{Zero}} to {{Hero}}},
  author = {Karpathy, Andrej},
  year = {2022},
  month = aug
}

@misc{karvonenEmergentWorldModels2024,
  title = {Emergent {{World Models}} and {{Latent Variable Estimation}} in {{Chess-Playing Language Models}}},
  author = {Karvonen, Adam},
  year = {2024},
  month = jul,
  number = {arXiv:2403.15498},
  eprint = {2403.15498},
  publisher = {arXiv},
  urldate = {2024-11-22},
  abstract = {Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character. We derive a player skill vector and add it to the model, improving the model's win rate by up to 2.6 times.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/vikas/Zotero/storage/K94QMFKX/Karvonen - 2024 - Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models.pdf;/Users/vikas/Zotero/storage/ES8VK47Q/2403.html}
}

@book{kevinp.murphyProbabilisticMachineLearning2022,
  title = {Probabilistic {{Machine Learning}}: {{An}} Introduction},
  author = {{Kevin P. Murphy}},
  year = {2022},
  publisher = {MIT Press},
  file = {/Users/vikasprasad/Downloads/Probabilistic Machine Learning - An Introduction.pdf}
}

@book{kevinp.murphyProbabilisticMachineLearning2023,
  title = {Probabilistic {{Machine Learning}}: {{Advanced Topics}}},
  author = {{Kevin P. Murphy}},
  year = {2023},
  publisher = {MIT Press},
  file = {/Users/vikasprasad/Downloads/Probabilistic Machine Learning - Advanced Topics.pdf}
}

@misc{kilianweinbergerCS457802024,
  title = {{{CS}} 4/5780: {{Intro}} to {{Machine Learning}}},
  author = {{Kilian Weinberger}},
  year = {2024}
}

@misc{kojimaLargeLanguageModels2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.11916},
  urldate = {2024-10-18},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{konradlorenzAnalogySourceKnowledge1973,
  title = {Analogy as a {{Source}} of {{Knowledge}}},
  author = {{Konrad Lorenz}},
  year = {1973},
  month = dec,
  address = {Stockholm, Sweden},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Konrad Lorenz - Nobel Lecture.pdf}
}

@misc{konradlorenzKonradLorenzBiography1973,
  title = {Konrad {{Lorenz}}: {{Biography}}},
  author = {{Konrad Lorenz}},
  year = {1973},
  month = dec
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.541},
  urldate = {2024-10-11},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  langid = {english}
}

@misc{liuBackstoryBackpropagation2023,
  title = {The {{Backstory}} of {{Backpropagation}}},
  author = {Liu, Yuxi},
  year = {2023},
  month = dec,
  journal = {Yuxi on the Wired},
  urldate = {2024-11-26},
  abstract = {Why backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.},
  howpublished = {https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/},
  langid = {english}
}

@book{mackayInformationTheoryInference2003,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = {2003},
  edition = {22nd printing},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  isbn = {978-0-521-64298-9},
  langid = {english}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  journal = {The Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {0007-4985, 1522-9602},
  doi = {10.1007/BF02478259},
  urldate = {2024-11-09},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/McCulloch.and.Pitts-1943.pdf}
}

@misc{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  shorttitle = {Word2vec},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1310.4546},
  urldate = {2024-10-12},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{neilsenVisualProofThat2019,
  title = {A Visual Proof That Neural Nets Can Compute Any Function},
  author = {Neilsen, Michael},
  year = {2019}
}

@book{nilssonQuestArtificialIntelligence2010,
  title = {The Quest for Artificial Intelligence: A History of Ideas and Achievements},
  shorttitle = {The Quest for Artificial Intelligence},
  author = {Nilsson, Nils J.},
  year = {2010},
  publisher = {Cambridge University Press},
  address = {Cambridge ; New York},
  isbn = {978-0-521-11639-8 978-0-521-12293-1},
  lccn = {Q335 .N55 2010},
  annotation = {OCLC: ocn396185265}
}

@misc{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1912.01703},
  urldate = {2024-10-11},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{patrickmckenzieAnatomyCreditCard2024,
  title = {Anatomy of a Credit Card Rewards Program},
  author = {{Patrick McKenzie}},
  year = {2024},
  month = mar,
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Anatomy of a credit card rewards program.pdf}
}

@article{peternaurProgrammingTheoryBuilding1985,
  title = {Programming as Theory Building},
  author = {{Peter Naur}},
  year = {1985},
  month = may,
  journal = {Microprocessing and Microprogramming},
  volume = {15},
  number = {5},
  pages = {253--261},
  issn = {01656074},
  doi = {10.1016/0165-6074(85)90032-8},
  urldate = {2024-10-09},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {to-note},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Naur - Programming as Theory Building.pdf}
}

@book{princeUnderstandingDeepLearning2023,
  title = {Understanding {{Deep Learning}}},
  author = {Prince, Simon J D},
  year = {2023},
  isbn = {978-0-262-04864-4},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/UnderstandingDeepLearning_01_10_24_C.pdf}
}

@misc{richpangAlternativeConstructionShannon2024,
  title = {An Alternative Construction of {{Shannon}} Entropy -- {{Rich Pang}}},
  author = {{Rich Pang}},
  year = {2024},
  month = aug,
  urldate = {2024-11-18},
  langid = {american},
  file = {/Users/vikas/Zotero/storage/ISZYJ7ST/an-alternative-construction-of-shannon-entropy.html}
}

@misc{richsuttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {{Rich Sutton}},
  year = {2019},
  month = mar
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2024-10-18},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@book{russellArtificialIntelligenceModern2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2021},
  series = {Pearson {{Series}} in {{Artificial Intelligence}}},
  edition = {Fourth Edition},
  publisher = {Pearson},
  address = {Hoboken, NJ},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  collaborator = {Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikash and Pearl, Judea and Wooldridge, Michael J.},
  isbn = {978-0-13-461099-3},
  langid = {english}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude},
  year = {1948},
  journal = {Bell System Technical Journal},
  volume = {27}
}

@article{shannonPredictionEntropyPrinted1951,
  title = {Prediction and {{Entropy}} of {{Printed English}}},
  author = {Shannon, Claude},
  year = {1951},
  month = jan,
  journal = {Bell System Technical Journal},
  volume = {30},
  number = {1},
  pages = {50--64},
  issn = {00058580},
  doi = {10.1002/j.1538-7305.1951.tb01366.x},
  urldate = {2024-10-18},
  langid = {english}
}

@misc{sinnLandauChessTournaments2022,
  title = {Landau on {{Chess Tournaments}} and {{Google}}'s {{PageRank}}},
  author = {Sinn, Rainer and Ziegler, G{\"u}nter M.},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.17300},
  urldate = {2024-11-02},
  abstract = {In his first mathematical paper, published in 1895 when he was 18, Edmund Landau suggested a new way to determine the winner of a chess tournament by not simply adding for each player the fixed number of points they would get for each win or draw, but rather by considering the performance of all players in the tournament relative to each other: each player would get more credit for games won or drawn against stronger players. Landau called this "relative Wertbemessung", which translates to relative score. The basic idea from linear algebra behind this scoring system was rediscovered and reused in many contexts since 1895; in particular, it is a central ingredient in Google's PageRank.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Landau on Chess Tournaments and PageRank.pdf.pdf}
}

@book{terrencej.sejnowskiDeepLearningRevolution2018,
  title = {The Deep Learning Revolution},
  author = {{Terrence J. Sejnowski}},
  year = {2018},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-03803-4},
  lccn = {Q325.5 .S45 2018},
  file = {/Users/vikasprasad/Downloads/The Deep Learning Revolution - Sejnowski.pdf}
}

@misc{touvronLlama2Open2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.09288},
  urldate = {2024-10-18},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ucberkeleyEECS20NSignalsSystems2011,
  title = {{{EECS20N}}: {{Signals}} and {{Systems}} - {{Modem Negotiation}}},
  author = {{UC Berkeley}},
  year = {2011}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1706.03762},
  urldate = {2024-10-18},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{yanaiRenaissanceMinds21st2020,
  title = {Renaissance Minds in 21st Century Science},
  author = {Yanai, Itai and Lercher, Martin},
  year = {2020},
  month = dec,
  journal = {Genome Biology},
  volume = {21},
  number = {1},
  pages = {67, s13059-020-01985-6},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-01985-6},
  urldate = {2024-11-03},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Renaissance Minds in 21st Century.pdf}
}
