@misc{3blue1brownEssenceLinearAlgebra2016,
  title = {Essence of {{Linear Algebra}}},
  shorttitle = {3blue1brown - Linear Algebra},
  author = {{3blue1brown}},
  year = {2016},
  month = aug,
  url = {https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}
}

@misc{3blue1brownNeuralNetworks2024,
  title = {Neural {{Networks}}},
  author = {{3blue1brown}},
  year = {2024},
  url = {https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}
}

@misc{AlgorithmIllustrated,
  title = {The {{DES Algorithm Illustrated}}},
  url = {https://page.math.tu-berlin.de/~kant/teaching/hess/krypto-ws2006/des.htm},
  urldate = {2025-01-05},
  file = {/Users/vikas/Zotero/storage/9Z6WLWWG/des.html}
}

@article{ameisen2025circuit,
  title = {Circuit Tracing: {{Revealing}} Computational Graphs in Language Models},
  author = {Ameisen, Emmanuel and Lindsey, Jack and Pearce, Adam and Gurnee, Wes and Turner, Nicholas L. and Chen, Brian and Citro, Craig and Abrahams, David and Carter, Shan and Hosmer, Basil and Marcus, Jonathan and Sklar, Michael and Templeton, Adly and Bricken, Trenton and McDougall, Callum and Cunningham, Hoagy and Henighan, Thomas and Jermyn, Adam and Jones, Andy and Persic, Andrew and Qi, Zhenyi and Ben Thompson, T. and Zimmerman, Sam and Rivoire, Kelley and Conerly, Thomas and Olah, Chris and Batson, Joshua},
  year = {2025},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2025/attribution-graphs/methods.html},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Circuit Tracing_ Revealing Computational Graphs in Language Models.pdf}
}

@book{ananthaswamyWhyMachinesLearn2024,
  title = {Why Machines Learn: The Elegant Math behind Modern {{AI}}},
  shorttitle = {Why Machines Learn},
  author = {Ananthaswamy, Anil},
  year = {2024},
  publisher = {Dutton},
  address = {New York},
  abstract = {"A rich, narrative explanation of the mathematics that has brought us machine learning and the ongoing explosion of artificial intelligence"--},
  isbn = {978-0-593-18574-2},
  lccn = {Q325.5 .A56 2024}
}

@misc{andrejkarpathyDeepDiveLLMs2025,
  title = {Deep {{Dive}} into {{LLMs}} like {{ChatGPT}}},
  author = {{Andrej Karpathy}},
  year = {2025},
  month = feb,
  url = {https://www.youtube.com/watch?v=7xTGNNLPyMI}
}

@misc{andrejkarpathyDeepReinforcementLearning2016,
  title = {Deep {{Reinforcement Learning}}: {{Pong}} from {{Pixels}}},
  author = {{Andrej Karpathy}},
  year = {2016},
  month = may,
  url = {https://karpathy.github.io/2016/05/31/rl/},
  urldate = {2025-07-01},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Deep Reinforcement Learning_ Pong from Pixels.pdf}
}

@misc{andrejkarpathySpelledoutIntroLanguage2022,
  title = {The Spelled-out Intro to Language Modeling: Building Makemore},
  author = {{Andrej Karpathy}},
  year = {2022},
  month = sep,
  url = {https://www.youtube.com/watch?v=PaCmpygFfXo}
}

@misc{andrejkarpathySpelledoutIntroNeural2022,
  title = {The Spelled-out Intro to Neural Networks and Backpropagation: Building Micrograd},
  author = {{Andrej Karpathy}},
  year = {2022},
  month = aug,
  url = {https://youtu.be/VMj-3S1tku0?si=ZdEa-gwxlrV6rhD7}
}

@misc{andrejkarpathyStateGPT2023,
  title = {State of {{GPT}}},
  author = {{Andrej Karpathy}},
  year = {2023},
  month = may,
  url = {https://www.youtube.com/watch?v=bZQun8Y4L2A}
}

@misc{andrejkarpathyUnreasonableEffectivenessRecurrent2015,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  author = {{Andrej Karpathy}},
  year = {2015},
  month = may,
  url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/}
}

@techreport{anthropicincClaude37Sonnet2025,
  title = {Claude 3.7 {{Sonnet System Card}}},
  author = {{Anthropic, Inc}},
  year = {2025},
  month = feb,
  url = {https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Claude 3.7 Sonnet System Card.pdf}
}

@techreport{anthropicincClaude3Model2024,
  title = {The {{Claude}} 3 {{Model Family}}: {{Opus}}, {{Sonnet}}, {{Haiku}}},
  author = {{Anthropic, Inc}},
  year = {2024},
  month = jun,
  url = {https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf}
}

@misc{anthropicincModelContextProtocol2024,
  title = {Model {{Context Protocol}} - {{Specification}}},
  author = {{Anthropic, Inc}},
  year = {2024},
  month = nov,
  url = {https://spec.modelcontextprotocol.io/specification/2024-11-05/}
}

@book{aumassonSeriousCryptographyPractical2018,
  title = {Serious Cryptography: A Practical Introduction to Modern Encryption},
  shorttitle = {Serious Cryptography},
  author = {Aumasson, Jean-Philippe},
  year = {2018},
  publisher = {No Starch Press},
  address = {San Francisco},
  abstract = {Encryption -- Randomness -- Cryptographic security -- Block ciphers -- Stream ciphers -- Hash functions -- Keyed hashing -- Authenticated encryption -- Hard problems -- RSA -- Diffie-Hellman -- Elliptic curves -- TLS -- Quantum and post-quantum},
  isbn = {978-1-59327-882-3},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/seriouscrytography.pdf}
}

@misc{bartosFairCoinsTend2023,
  title = {Fair Coins Tend to Land on the Same Side They Started: {{Evidence}} from 350,757 Flips},
  shorttitle = {Fair Coins Tend to Land on the Same Side They Started},
  author = {Barto{\v s}, Franti{\v s}ek and Sarafoglou, Alexandra and Godmann, Henrik R. and Sahrani, Amir and Leunk, David Klein and Gui, Pierre Y. and Voss, David and Ullah, Kaleem and Zoubek, Malte J. and Nippold, Franziska and Aust, Frederik and Vieira, Felipe F. and Islam, Chris-Gabriel and Zoubek, Anton J. and Shabani, Sara and Petter, Jonas and Roos, Ingeborg B. and Finnemann, Adam and Lob, Aaron B. and Hoffstadt, Madlen F. and Nak, Jason and {de Ron}, Jill and Derks, Koen and Huth, Karoline and Terpstra, Sjoerd and Bastelica, Thomas and Matetovici, Magda and Ott, Vincent L. and Zetea, Andreea S. and Karnbach, Katharina and Donzallaz, Michelle C. and John, Arne and Moore, Roy M. and Assion, Franziska and {van Bork}, Riet and Leidinger, Theresa E. and Zhao, Xiaochang and Motaghi, Adrian Karami and Pan, Ting and Armstrong, Hannah and Peng, Tianqi and Bialas, Mara and Pang, Joyce Y. -C. and Fu, Bohan and Yang, Shujun and Lin, Xiaoyi and Sleiffer, Dana and Bognar, Miklos and Aczel, Balazs and Wagenmakers, Eric-Jan},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.04153},
  url = {https://arxiv.org/abs/2310.04153},
  urldate = {2024-11-19},
  abstract = {Many people have flipped coins but few have stopped to ponder the statistical and physical intricacies of the process. In a preregistered study we collected \$350\{,\}757\$ coin flips to test the counterintuitive prediction from a physics model of human coin tossing developed by Diaconis, Holmes, and Montgomery (DHM; 2007). The model asserts that when people flip an ordinary coin, it tends to land on the same side it started -- DHM estimated the probability of a same-side outcome to be about 51\%. Our data lend strong support to this precise prediction: the coins landed on the same side more often than not, \${\textbackslash}text\{Pr\}({\textbackslash}text\{same side\}) = 0.508\$, 95\% credible interval (CI) [\$0.506\$, \$0.509\$], \${\textbackslash}text\{BF\}\_\{{\textbackslash}text\{same-side bias\}\} = 2359\$. Furthermore, the data revealed considerable between-people variation in the degree of this same-side bias. Our data also confirmed the generic prediction that when people flip an ordinary coin -- with the initial side-up randomly determined -- it is equally likely to land heads or tails: \${\textbackslash}text\{Pr\}({\textbackslash}text\{heads\}) = 0.500\$, 95\% CI [\$0.498\$, \$0.502\$], \${\textbackslash}text\{BF\}\_\{{\textbackslash}text\{heads-tails bias\}\} = 0.182\$. Furthermore, this lack of heads-tails bias does not appear to vary across coins. Additional exploratory analyses revealed that the within-people same-side bias decreased as more coins were flipped, an effect that is consistent with the possibility that practice makes people flip coins in a less wobbly fashion. Our data therefore provide strong evidence that when some (but not all) people flip a fair coin, it tends to land on the same side it started. Our data provide compelling statistical support for the DHM physics model of coin tossing.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/350757Flips.pdf}
}

@article{baydinAutomaticDifferentiationMachine2015,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  shorttitle = {{{AD}} Survey},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2015},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1502.05767},
  url = {https://arxiv.org/abs/1502.05767},
  urldate = {2024-10-11},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  year = {2003},
  month = mar,
  journal = {J. Mach. Learn. Res.},
  volume = {3},
  number = {null},
  pages = {1137--1155},
  publisher = {JMLR.org},
  issn = {1532-4435},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/A Neural Probabalistic Language Model.pdf}
}

@article{bentleyProgrammingPearlsLiterate1986,
  title = {Programming Pearls: A Literate Program},
  shorttitle = {Programming Pearls},
  author = {Bentley, Jon and Knuth, Don and McIlroy, Doug},
  year = {1986},
  month = jun,
  journal = {Communications of the ACM},
  volume = {29},
  number = {6},
  pages = {471--483},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/5948.315654},
  url = {https://dl.acm.org/doi/10.1145/5948.315654},
  urldate = {2025-01-18},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/a-literate-program-bentley.pdf}
}

@book{bishopDeepLearningFoundations2024,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  year = {2024},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  url = {https://link.springer.com/10.1007/978-3-031-45468-4},
  urldate = {2025-05-06},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Deep Learning Foundations and Concepts - Bishop.pdf}
}

@article{blackNoise1986,
  title = {Noise},
  author = {Black, Fischer},
  year = {1986},
  month = jul,
  journal = {The Journal of Finance},
  volume = {41},
  number = {3},
  pages = {528--543},
  issn = {0022-1082, 1540-6261},
  doi = {10.1111/j.1540-6261.1986.tb04513.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.1986.tb04513.x},
  urldate = {2025-05-29},
  abstract = {ABSTRACT             The effects of noise on the world, and on our views of the world, are profound. Noise in the sense of a large number of small events is often a causal factor much more powerful than a small number of large events can be. Noise makes trading in financial markets possible, and thus allows us to observe prices for financial assets. Noise causes markets to be somewhat inefficient, but often prevents us from taking advantage of inefficiencies. Noise in the form of uncertainty about future tastes and technology by sector causes business cycles, and makes them highly resistant to improvement through government intervention. Noise in the form of expectations that need not follow rational rules causes inflation to be what it is, at least in the absence of a gold standard or fixed exchange rates. Noise in the form of uncertainty about what relative prices would be with other exchange rates makes us think incorrectly that changes in exchange rates or inflation rates cause changes in trade or investment flows or economic activity. Most generally, noise makes it very difficult to test either practical or academic theories about the way that financial or economic markets work. We are forced to act largely in the dark.},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Fischer Black - 1986 - Noise.pdf}
}

@book{blitzsteinIntroductionProbability2019,
  title = {Introduction to Probability},
  author = {Blitzstein, Joseph K. and Hwang, Jessica},
  year = {2019},
  edition = {Second edition},
  publisher = {CRC Press},
  address = {Boca Raton},
  abstract = {"Undergraduate probability book that assumes one-semester of calculus. One key is the emphasis on "stories" for the probability distributions (which I mean in both an intuitive and technical sense): there are a dozen or so key distributions (Normal, Binomial, Poisson, etc.) that are incredibly widely-used in statistics, but a lot of books just write down formulas for them without explaining clearly why these particular distributions are so important, or how they are all connected. Each of these distributions has a "story" (a natural application where it arises), and thinking about stories makes the distributions easier to remember, understand, and work with"--},
  isbn = {978-0-429-76673-2 978-0-429-42835-7},
  lccn = {QA273},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/Introduction to Probability by Joseph K. Blitzstein, Jessica Hwang (z-lib.org).pdf}
}

@article{blitzsteinSolutionsIntroductionProbability,
  title = {Solutions - {{Introduction}} to {{Probability}}},
  author = {Blitzstein, Joseph K and Hwang, Jessica},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/selected_solutions_blitzstein_hwang_probability_01.pdf}
}

@article{blitzsteinStat110Strategic,
  title = {Stat 110 {{Strategic Practice}} 3},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_3.pdf}
}

@article{blitzsteinStat110Strategica,
  title = {Stat 110 {{Strategic Practice}} 11},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_11.pdf}
}

@article{blitzsteinStat110Strategicb,
  title = {Stat 110 {{Strategic Practice}} 10},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_10.pdf}
}

@article{blitzsteinStat110Strategicc,
  title = {Stat 110 {{Strategic Practice}} 5},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_5.pdf}
}

@article{blitzsteinStat110Strategicd,
  title = {Stat 110 {{Strategic Practice}} 6},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_6.pdf}
}

@article{blitzsteinStat110Strategice,
  title = {Stat 110 {{Strategic Practice}} 7},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_7.pdf}
}

@article{blitzsteinStat110Strategicf,
  title = {Stat 110 {{Strategic Practice}} 8},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_8.pdf}
}

@article{blitzsteinStat110Strategicg,
  title = {Stat 110 {{Strategic Practice}} 9},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_9.pdf}
}

@article{blitzsteinStat110Strategich,
  title = {Stat 110 {{Strategic Practice}} 4},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_4.pdf}
}

@article{blitzsteinStat110Strategici,
  title = {Stat 110 {{Strategic Practice}} 2},
  author = {Blitzstein, Joe},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_2.pdf}
}

@article{blitzsteinStat110Strategicj,
  title = {Stat 110 {{Strategic Practice}} 1},
  author = {Blitzstein, Joe},
  journal = {A man},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/stats110/strategic_practice_and_homework_1.pdf}
}

@techreport{blumeventuresIndusValleyAnnual2025,
  title = {Indus {{Valley Annual Report}} 2025},
  author = {{Blume Ventures}},
  year = {2025},
  month = feb,
  pages = {188},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Indus Valley Annual Report 2025.pdf}
}

@inproceedings{boserTrainingAlgorithmOptimal1992,
  title = {A Training Algorithm for Optimal Margin Classifiers},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  year = {1992},
  month = jul,
  pages = {144--152},
  publisher = {ACM},
  address = {Pittsburgh Pennsylvania USA},
  doi = {10.1145/130385.130401},
  url = {https://dl.acm.org/doi/10.1145/130385.130401},
  urldate = {2024-11-09},
  isbn = {978-0-89791-497-0},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/A Training Algorithm for Optimal Margin Classifiers - Boser 1992.pdf}
}

@misc{bottouOptimizationMethodsLargeScale2016,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1606.04838},
  url = {https://arxiv.org/abs/1606.04838},
  urldate = {2024-10-18},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{breimanReflectionsRefereeingPapers1995,
  title = {Reflections {{After Refereeing Papers}} for {{NIPS}}},
  booktitle = {The {{Mathematics}} of {{Generalization}}},
  author = {Breiman, Leo},
  year = {1995},
  edition = {1},
  pages = {11--15},
  publisher = {CRC Press},
  doi = {10.1201/9780429492525-2},
  url = {https://www.taylorfrancis.com/books/9780429961076/chapters/10.1201/9780429492525-2},
  urldate = {2024-11-27},
  collaborator = {Wolpert, David. H},
  isbn = {978-0-429-49252-5},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Breiman - 2018 - Reflections After Refereeing Papers for NIPS.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  urldate = {2024-10-12},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{burkemanFreeYourMind2024,
  title = {Free {{Your Mind}}. {{The Election Will Follow}}.},
  author = {Burkeman, Oliver},
  year = {2024},
  month = nov,
  journal = {The New York Times},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Opinion _ Free Your Mind. The Election Will Follow. - The New York Times.pdf}
}

@misc{chintalaHowTrainModel2024,
  title = {How to Train a Model on 10k {{H100 GPUs}}?},
  author = {Chintala, Soumith},
  year = {2024},
  month = oct,
  url = {https://soumith.ch/blog/2024-10-02-training-10k-scale.md.html}
}

@book{clarkeHowWorldWas1992,
  title = {How the World Was One: Beyond the Global Village},
  shorttitle = {How the World Was One},
  author = {Clarke, Arthur C.},
  year = {1992},
  publisher = {Bantam Books},
  address = {New York},
  isbn = {978-0-553-07440-6},
  langid = {english}
}

@article{coutrotGlobalDeterminantsNavigation2018,
  title = {Global {{Determinants}} of {{Navigation Ability}}},
  author = {Coutrot, Antoine and Silva, Ricardo and Manley, Ed and De Cothi, Will and Sami, Saber and Bohbot, V{\'e}ronique D. and Wiener, Jan M. and H{\"o}lscher, Christoph and Dalton, Ruth C. and Hornberger, Michael and Spiers, Hugo J.},
  year = {2018},
  month = sep,
  journal = {Current Biology},
  volume = {28},
  number = {17},
  pages = {2861-2866.e4},
  issn = {09609822},
  doi = {10.1016/j.cub.2018.06.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982218307711},
  urldate = {2025-02-05},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Global Determinants of Navigation Ability.pdf}
}

@book{coverElementsInformationTheory2005,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2005},
  month = sep,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/047174882X},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X},
  urldate = {2024-10-18},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-0-471-24195-9 978-0-471-74882-3},
  langid = {english}
}

@article{davida.pattersonLifeLessonsFirst2024,
  title = {Life {{Lessons}} from the {{First Half-Century}} of {{My Career}}},
  author = {{David A. Patterson}},
  year = {2024},
  month = oct,
  journal = {Communications of the ACM},
  pages = {3637905},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3637905},
  url = {https://dl.acm.org/doi/10.1145/3637905},
  urldate = {2024-10-12},
  langid = {english}
}

@misc{dubeyLlama3Herd2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and {van der Linde}, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and {Rantala-Yeary}, Lauren and {van der Maaten}, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and {de Oliveira}, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzm{\'a}n, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, V{\'i}tor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and {Wang} and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2407.21783},
  url = {https://arxiv.org/abs/2407.21783},
  urldate = {2024-10-18},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DummysGuideModern2024,
  title = {Dummy's {{Guide}} to {{Modern LLM Sampling}}},
  year = {2024},
  month = mar,
  url = {https://rentry.co/samplers},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Dummys Guide to Modern LLM Sampling.pdf}
}

@book{edgertonSickSocietiesChallenging1992,
  title = {Sick Societies: Challenging the Myth of Primitive Harmony},
  shorttitle = {Sick Societies},
  author = {Edgerton, Robert B.},
  year = {1992},
  publisher = {Free Press ; Maxwell Macmillan Canada ; Maxwell Macmillan International},
  address = {New York : Toronto : New York},
  isbn = {978-0-02-908925-5},
  lccn = {GN345.5 .E34 1992}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  year = {1990},
  month = mar,
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {0364-0213, 1551-6709},
  doi = {10.1207/s15516709cog1402_1},
  url = {https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1},
  urldate = {2024-10-18},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english}
}

@misc{fips463,
  title = {Data Encryption Standard ({{DES}})},
  author = {{National Institute of Standards and Technology}},
  year = {1999},
  month = oct,
  added-at = {2009-01-14T00:43:43.000+0100},
  description = {dret'd bibliography},
  howpublished = {FIPS Publication 46-3},
  interhash = {e345f8a7b3fae7846a7f76f984862d01},
  intrahash = {cc9f8a97c52014a952c4426c7f578b13},
  topic = {des[1] dea[1] tripledes[1] tdea[1]},
  updates = {des},
  keywords = {imported},
  timestamp = {2009-01-14T00:44:03.000+0100},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Data Encryption Standard FIPS 46-3.pdf}
}

@misc{francoislabelleEloWinProbability2016,
  title = {Elo {{Win Probability Calculator}}},
  author = {{Fran{\c c}ois Labelle}},
  year = {2016},
  month = sep,
  url = {https://wismuth.com/elo/calculator.html},
  urldate = {2025-01-01},
  file = {/Users/vikas/Zotero/storage/M3AUS7SY/calculator.html}
}

@book{gertnerIdeaFactoryBell2013,
  title = {The Idea Factory: {{Bell Labs}} and the Great Age of {{American}} Innovation},
  shorttitle = {The Idea Factory},
  author = {Gertner, Jon},
  year = {2013},
  publisher = {Penguin Books},
  address = {London},
  isbn = {978-0-14-312279-1 978-1-59420-328-2},
  langid = {english}
}

@book{gurriRevoltPublicCrisis2018,
  title = {The Revolt of the Public and the Crisis of Authority in the New Millennium},
  author = {Gurri, Martin},
  year = {2018},
  edition = {Second edition},
  publisher = {Stripe Press},
  address = {San Francisco, CA},
  abstract = {In the words of economist and scholar Arnold Kling, "Martin Gurri saw it coming." Technology has categorically reversed the information balance of power between the public and the elites who manage the great hierarchical institutions of the industrial age--government, political parties, the media. The Revolt of the Public tells the story of how insurgencies, enabled by digital devices and a vast information sphere, have mobilized millions of ordinary people around the world. Originally published in 2014, this updated edition of The Revolt of the Public includes an extensive analysis of Donald Trump's improbable rise to the presidency and the electoral triumphs of "Brexit" and concludes with a speculative look forward, pondering whether the current elite class can bring about a reformation of the democratic process, and whether new organizing principles, adapted to a digital world, can arise out of the present political turbulence. -- publisher's website},
  isbn = {978-1-7322651-4-1},
  lccn = {HM851 .G87 2018},
  annotation = {OCLC: on1081335036}
}

@book{hammingArtProbabilityScientists1991,
  title = {The {{Art}} of {{Probability}}: {{For Scientists}} and {{Engineers}}},
  shorttitle = {The {{Art}} of {{Probability}}},
  author = {Hamming, Richard W.},
  year = {1991},
  edition = {1},
  publisher = {CRC Press},
  doi = {10.1201/9780429492952},
  url = {https://www.taylorfrancis.com/books/9780429961502},
  urldate = {2024-10-12},
  isbn = {978-0-429-49295-2},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/The Art of Probability for Scientists and Engineers.pdf}
}

@article{hangliLanguageModelsPresent2022,
  title = {Language Models: Past, Present, and Future},
  shorttitle = {History of Language Modeling},
  author = {{Hang Li}},
  year = {2022},
  month = jul,
  journal = {Communications of the ACM},
  volume = {65},
  number = {7},
  pages = {56--63},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3490443},
  url = {https://dl.acm.org/doi/10.1145/3490443},
  urldate = {2024-10-07},
  abstract = {A language modeling overview, highlighting basic concepts, intuitive explanations, technical achievements, and fundamental challenges.},
  langid = {english},
  keywords = {note-published},
  file = {/Users/vikasprasad/Downloads/Hang Li 2022 - Language Models Past Present.pdf}
}

@techreport{Hay1960MarkIP,
  title = {Mark {{I}} Perceptron Operators' Manual},
  author = {Hay, John C. and Lynch, B. and Smith, David Russell Bedford},
  year = {1960},
  url = {https://apps.dtic.mil/sti/tr/pdf/AD0236965.pdf},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Mark I Perceptron operators manual.pdf}
}

@article{hayesFirstLinksMarkov2013,
  title = {First {{Links}} in the {{Markov Chain}}},
  author = {Hayes, Brian},
  year = {2013},
  journal = {American Scientist},
  volume = {101},
  number = {2},
  url = {https://doi.org/10.1511/2013.101.92}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2203.15556},
  url = {https://arxiv.org/abs/2203.15556},
  urldate = {2024-10-18},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J J},
  year = {1982},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.79.8.2554},
  url = {https://pnas.org/doi/full/10.1073/pnas.79.8.2554},
  urldate = {2024-11-20},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/hopfield-1982-neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.pdf}
}

@article{horganClaudeShannonProfile1992,
  title = {Claude {{E}}. {{Shannon}} [{{Profile}}]},
  author = {Horgan, J.},
  year = {1992},
  month = apr,
  journal = {IEEE Spectrum},
  volume = {29},
  number = {4},
  pages = {72--75},
  issn = {0018-9235, 1939-9340},
  doi = {10.1109/MSPEC.1992.672257},
  url = {https://spectrum.ieee.org/claude-shannon-tinkerer-prankster-and-father-of-information-theory},
  urldate = {2024-10-18},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@book{hwuProgrammingMassivelyParallel2023,
  title = {Programming Massively Parallel Processors: A Hands-on Approach},
  shorttitle = {Programming Massively Parallel Processors},
  author = {Hwu, Wen-mei W. and Kirk, David and El Hajj, Izzat},
  year = {2023},
  edition = {Fourth edition},
  publisher = {Morgan Kaufmann, Elsevier},
  address = {Cambridge, MA},
  doi = {10.1016/C2020-0-02969-5},
  abstract = {Programming Massively Parallel Processors: A Hands-on Approach shows both students and professionals alike the basic concepts of parallel programming and GPU architecture. Concise, intuitive, and practical, it is based on years of road-testing in the authors' own parallel computing courses. Various techniques for constructing and optimizing parallel programs are explored in detail, while case studies demonstrate the development process, which begins with computational thinking and ends with effective and efficient parallel programs. The new edition includes updated coverage of CUDA, including the newer libraries such as CuDNN. New chapters on frequently used parallel patterns have been added, and case studies have been updated to reflect current industry practices},
  isbn = {978-0-323-91231-0 978-0-323-98463-8},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Programming Massively Parallel Processors 4th ed.pdf}
}

@inproceedings{inproceedings,
  title = {{{UNIX}} Password Encryption Considered Insecure.},
  author = {Leong, Philip and Tham, C.},
  year = {1991},
  month = jan,
  pages = {269--280},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/UNIX_Password_Encryption_Considered_Insecure.pdf}
}

@misc{jackmorrisThereAreNo2025,
  title = {There {{Are No New Ideas}} in {{AI}}{\dots} {{Only New Datasets}}},
  author = {{Jack Morris}},
  year = {2025},
  month = apr,
  url = {https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only}
}

@misc{jeremykunGooglesPageRankFirst2011,
  title = {Google's {{PageRank}}---{{A First Attempt}}},
  author = {{Jeremy Kun}},
  year = {2011},
  month = jun,
  journal = {Math {$\cap$} Programming},
  url = {https://www.jeremykun.com/2011/06/18/googles-pagerank-a-first-attempt/},
  urldate = {2025-01-03},
  abstract = {The Web as a Graph The goal of this post is to assign an ``importance score'' \$ x\_i {\textbackslash}in [0,1]\$ to each of a set of web pages indexed \$ v\_i\$ in a way that consistently captures our idea of which websites are likely to be important. But before we can extract information from the structure of the internet, we need to have a mathematical description of that structure. Enter graph theory.},
  chapter = {posts},
  langid = {american}
}

@misc{jeremykunMarkovChainMonte2015,
  title = {Markov {{Chain Monte Carlo Without}} All the {{Bullshit}}},
  author = {{Jeremy Kun}},
  year = {2015},
  month = apr,
  url = {https://www.jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/}
}

@book{jeremykunProgrammersIntroductionMathematics2021,
  title = {A {{Programmer}}'s {{Introduction}} to {{Mathematics}}},
  author = {{Jeremy Kun}},
  year = {2021},
  url = {https://pimbook.org/}
}

@misc{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.06825},
  url = {https://arxiv.org/abs/2310.06825},
  urldate = {2024-10-18},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  copyright = {Creative Commons Attribution 4.0 International}
}

@book{jm3,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2025},
  edition = {3rd},
  url = {https://web.stanford.edu/~jurafsky/slp3/}
}

@misc{johnc.baezWhatEntropy2024,
  title = {What Is {{Entropy}}?},
  author = {{John C. Baez}},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2409.09232},
  url = {https://arxiv.org/abs/2409.09232},
  urldate = {2024-10-12},
  abstract = {This short book is an elementary course on entropy, leading up to a calculation of the entropy of hydrogen gas at standard temperature and pressure. Topics covered include information, Shannon entropy and Gibbs entropy, the principle of maximum entropy, the Boltzmann distribution, temperature and coolness, the relation between entropy, expected energy and temperature, the equipartition theorem, the partition function, the relation between expected energy, free energy and entropy, the entropy of a classical harmonic oscillator, the entropy of a classical particle in a box, and the entropy of a classical ideal gas.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{johnrauserStatisticsAgonizingPain2014,
  title = {Statistics without the Agonizing Pain},
  author = {{John Rauser}},
  year = {2014},
  month = oct,
  url = {https://www.youtube.com/watch?v=5Dnw46eC-0o}
}

@article{jordaRateReturnEverything2019,
  title = {The {{Rate}} of {{Return}} on {{Everything}}, 1870--2015*},
  author = {Jord{\`a}, {\`O}scar and Knoll, Katharina and Kuvshinov, Dmitry and Schularick, Moritz and Taylor, Alan M},
  year = {2019},
  month = aug,
  journal = {The Quarterly Journal of Economics},
  volume = {134},
  number = {3},
  pages = {1225--1298},
  issn = {0033-5533, 1531-4650},
  doi = {10.1093/qje/qjz012},
  url = {https://academic.oup.com/qje/article/134/3/1225/5435538},
  urldate = {2024-11-03},
  abstract = {Abstract             What is the aggregate real rate of return in the economy? Is it higher than the growth rate of the economy and, if so, by how much? Is there a tendency for returns to fall in the long run? Which particular assets have the highest long-run returns? We answer these questions on the basis of a new and comprehensive data set for all major asset classes, including housing. The annual data on total returns for equity, housing, bonds, and bills cover 16 advanced economies from 1870 to 2015, and our new evidence reveals many new findings and puzzles.},
  copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Jorda - The Rate of Return on Everything.pdf}
}

@misc{karpathyDeepNeuralNets2022,
  title = {Deep {{Neural Nets}}: 33 Years Ago and 33 Years from Now},
  author = {Karpathy, Andrej},
  year = {2022},
  month = mar,
  url = {https://karpathy.github.io/2022/03/14/lecun1989/}
}

@misc{karpathyMakemore2022,
  title = {Makemore},
  author = {Karpathy, Andrej},
  year = {2022},
  month = jun,
  url = {https://github.com/karpathy/makemore},
  copyright = {MIT License}
}

@misc{karpathyNeuralNetworksZero2022,
  title = {Neural {{Networks}}: {{Zero}} to {{Hero}}},
  author = {Karpathy, Andrej},
  year = {2022},
  month = aug,
  url = {https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ}
}

@misc{karvonenEmergentWorldModels2024,
  title = {Emergent {{World Models}} and {{Latent Variable Estimation}} in {{Chess-Playing Language Models}}},
  author = {Karvonen, Adam},
  year = {2024},
  month = jul,
  number = {arXiv:2403.15498},
  eprint = {2403.15498},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.15498},
  urldate = {2024-11-22},
  abstract = {Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character. We derive a player skill vector and add it to the model, improving the model's win rate by up to 2.6 times.},
  archiveprefix = {arXiv},
  file = {/Users/vikas/Zotero/storage/K94QMFKX/Karvonen - 2024 - Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models.pdf;/Users/vikas/Zotero/storage/ES8VK47Q/2403.html}
}

@book{kevinp.murphyProbabilisticMachineLearning2022,
  title = {Probabilistic {{Machine Learning}}: {{An}} Introduction},
  author = {{Kevin P. Murphy}},
  year = {2022},
  publisher = {MIT Press},
  url = {probml.ai},
  file = {/Users/vikasprasad/Downloads/Probabilistic Machine Learning - An Introduction.pdf}
}

@book{kevinp.murphyProbabilisticMachineLearning2023,
  title = {Probabilistic {{Machine Learning}}: {{Advanced Topics}}},
  author = {{Kevin P. Murphy}},
  year = {2023},
  publisher = {MIT Press},
  url = {http://probml.github.io/book2},
  file = {/Users/vikasprasad/Downloads/Probabilistic Machine Learning - Advanced Topics.pdf}
}

@misc{kilianweinbergerCS457802024,
  title = {{{CS}} 4/5780: {{Intro}} to {{Machine Learning}}},
  author = {{Kilian Weinberger}},
  year = {2024},
  url = {https://www.cs.cornell.edu/courses/cs4780/2024sp/}
}

@article{kirbyCoffeeCanPortfolio1984,
  title = {The {{Coffee Can}} Portfolio},
  author = {Kirby, Robert G.},
  year = {1984},
  month = oct,
  journal = {The Journal of Portfolio Management},
  volume = {11},
  number = {1},
  pages = {76--80},
  issn = {0095-4918, 2168-8656},
  doi = {10.3905/jpm.1984.408988},
  url = {http://pm-research.com/lookup/doi/10.3905/jpm.1984.408988},
  urldate = {2025-01-22},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/The Coffee Can Portfolio.pdf}
}

@article{knuthBreakingParagraphsLines1981,
  title = {Breaking Paragraphs into Lines},
  author = {Knuth, Donald E. and Plass, Michael F.},
  year = {1981},
  month = nov,
  journal = {Software: Practice and Experience},
  volume = {11},
  number = {11},
  pages = {1119--1184},
  issn = {0038-0644, 1097-024X},
  doi = {10.1002/spe.4380111102},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/spe.4380111102},
  urldate = {2025-07-22},
  abstract = {Abstract             This paper discusses a new approach to the problem of dividing the text of a paragraph into lines of approximately equal length. Instead of simply making decisions one line at a time, the method considers the paragraph as a whole, so that the final appearance of a given line might be influenced by the text on succeeding lines. A system based on three simple primitive concepts called `boxes', `glue', and `penalties' provides the ability to deal satisfactorily with a wide variety of typesetting problems in a unified framework, using a single algorithm that determines optimum breakpoints. The algorithm avoids backtracking by a judicious use of the techniques of dynamic programming. Extensive computational experience confirms that the approach is both efficient and effective in producing high-quality output. The paper concludes with a brief history of line-breaking methods, and an appendix presents a simplified algorithm that requires comparatively few resources.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english}
}

@misc{kojimaLargeLanguageModels2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.11916},
  url = {https://arxiv.org/abs/2205.11916},
  urldate = {2024-10-18},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{konradlorenzAnalogySourceKnowledge1973,
  title = {Analogy as a {{Source}} of {{Knowledge}}},
  author = {{Konrad Lorenz}},
  year = {1973},
  month = dec,
  address = {Stockholm, Sweden},
  url = {https://www.nobelprize.org/uploads/2018/06/lorenz-lecture.pdf},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Konrad Lorenz - Nobel Lecture.pdf}
}

@misc{konradlorenzKonradLorenzBiography1973,
  title = {Konrad {{Lorenz}}: {{Biography}}},
  author = {{Konrad Lorenz}},
  year = {1973},
  month = dec,
  url = {https://www.nobelprize.org/prizes/medicine/1973/lorenz/biographical/}
}

@article{kozuchPigKidneyEpithelial1975,
  title = {Pig Kidney Epithelial ({{PS}}) Cells: A Perfect Tool for the Study of Flaviviruses and Some Other Arboviruses},
  shorttitle = {Pig Kidney Epithelial ({{PS}}) Cells},
  author = {Kozuch, O. and Mayer, V.},
  year = {1975},
  month = nov,
  journal = {Acta Virologica},
  volume = {19},
  number = {6},
  pages = {498},
  issn = {0001-723X},
  langid = {english},
  pmid = {1999}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}
}

@article{lazebnikCanBiologistFix2002,
  title = {Can a Biologist Fix a Radio?---{{Or}}, What {{I}} Learned While Studying Apoptosis},
  shorttitle = {Can a Biologist Fix a Radio?},
  author = {Lazebnik, Yuri},
  year = {2002},
  month = sep,
  journal = {Cancer Cell},
  volume = {2},
  number = {3},
  pages = {179--182},
  issn = {15356108},
  doi = {10.1016/S1535-6108(02)00133-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1535610802001332},
  urldate = {2025-03-23},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Can a Biologist Fix a Radio.pdf}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.541},
  url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
  urldate = {2024-10-11},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  langid = {english}
}

@article{lecunffDistractibilityImpulsivityADHD2024,
  title = {Distractibility and {{Impulsivity}} in {{ADHD}} as an {{Evolutionary Mismatch}} of {{High Trait Curiosity}}},
  author = {Le Cunff, Anne-Laure},
  year = {2024},
  month = aug,
  journal = {Evolutionary Psychological Science},
  volume = {10},
  number = {3},
  pages = {282--297},
  issn = {2198-9885},
  doi = {10.1007/s40806-024-00400-8},
  url = {https://link.springer.com/10.1007/s40806-024-00400-8},
  urldate = {2025-02-28},
  abstract = {Abstract             Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental condition characterized by symptoms that include inattention, hyperactivity, and impulsivity. Recent research suggests that individuals with ADHD might exhibit higher levels of curiosity, which may be linked to their tendencies toward distractibility and impulsivity. This paper proposes an evolutionary mismatch hypothesis for high trait curiosity in ADHD, positing that `hypercuriosity', which may have been adaptive in ancestral environments characterized by scarce resources and unpredictable risks, has become mismatched in industrialized societies where environments are more stable and information rich. The theory predicts that individuals with ADHD will demonstrate heightened levels of novelty-seeking and exploratory behaviors, manifesting as symptoms labeled as distractibility and impulsivity in modern environments. The paper explores the potential evolutionary benefits of high trait curiosity, the consequences of an evolutionary mismatch, and the implications for research and practice. The limitations of the theory are addressed, such as the need for more targeted research on curiosity in ADHD and potential differences among ADHD subtypes. Future research directions are proposed to refine and test the hypothesis, ultimately contributing to a more nuanced understanding of ADHD and informing the development of strength-based interventions. This theoretical framework offers a novel perspective on the adaptive value of ADHD traits and their manifestation in modern societies.},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Distractibility and Impulsivity in ADHD as an Evolutionary Mismatch of High Trait Curiosity.pdf}
}

@article{lefevreBeerConsumptionIncreases2010,
  title = {Beer {{Consumption Increases Human Attractiveness}} to {{Malaria Mosquitoes}}},
  author = {Lef{\`e}vre, Thierry and Gouagna, Louis-Cl{\'e}ment and Dabir{\'e}, Kounbobr Roch and Elguero, Eric and Fontenille, Didier and Renaud, Fran{\c c}ois and Costantini, Carlo and Thomas, Fr{\'e}d{\'e}ric},
  editor = {Tregenza, Tom},
  year = {2010},
  month = mar,
  journal = {PLoS ONE},
  volume = {5},
  number = {3},
  pages = {e9546},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0009546},
  url = {https://dx.plos.org/10.1371/journal.pone.0009546},
  urldate = {2025-03-18},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Beer Consumption Increases Human Attractiveness to Malaria Mosquitoes.pdf}
}

@misc{lehmanSurprisingCreativityDigital2019,
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  shorttitle = {The {{Surprising Creativity}} of {{Digital Evolution}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'e}noy, Antoine and Gagn{\'e}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c c}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
  year = {2019},
  month = nov,
  number = {arXiv:1803.03453},
  eprint = {1803.03453},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.03453},
  url = {http://arxiv.org/abs/1803.03453},
  urldate = {2025-03-24},
  abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
  archiveprefix = {arXiv},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/The Surprising Creativity of Digital Evolution- A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities.pdf}
}

@misc{lemireFastRandomInteger2018,
  title = {Fast {{Random Integer Generation}} in an {{Interval}}},
  author = {Lemire, Daniel},
  year = {2018},
  month = dec,
  number = {arXiv:1805.10941},
  eprint = {1805.10941},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.10941},
  url = {http://arxiv.org/abs/1805.10941},
  urldate = {2024-11-30},
  abstract = {In simulations, probabilistic algorithms and statistical tests, we often generate random integers in an interval (e.g., [0,s)). For example, random integers in an interval are essential to the Fisher-Yates random shuffle. Consequently, popular languages like Java, Python, C++, Swift and Go include ranged random integer generation functions as part of their runtime libraries. Pseudo-random values are usually generated in words of a fixed number of bits (e.g., 32 bits, 64 bits) using algorithms such as a linear congruential generator. We need functions to convert such random words to random integers in an interval ([0,s)) without introducing statistical biases. The standard functions in programming languages such as Java involve integer divisions. Unfortunately, division instructions are relatively expensive. We review an unbiased function to generate ranged integers from a source of random words that avoids integer divisions with high probability. To establish the practical usefulness of the approach, we show that this algorithm can multiply the speed of unbiased random shuffling on x64 processors. Our proposed approach has been adopted by the Go language for its implementation of the shuffle function.},
  archiveprefix = {arXiv},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Lemire - 2018 - Fast Random Integer Generation in an Interval.pdf;/Users/vikas/Zotero/storage/ZSUS2DRI/1805.html}
}

@article{lindsey2025biology,
  title = {On the Biology of a Large Language Model},
  author = {Lindsey, Jack and Gurnee, Wes and Ameisen, Emmanuel and Chen, Brian and Pearce, Adam and Turner, Nicholas L. and Citro, Craig and Abrahams, David and Carter, Shan and Hosmer, Basil and Marcus, Jonathan and Sklar, Michael and Templeton, Adly and Bricken, Trenton and McDougall, Callum and Cunningham, Hoagy and Henighan, Thomas and Jermyn, Adam and Jones, Andy and Persic, Andrew and Qi, Zhenyi and Thompson, T. Ben and Zimmerman, Sam and Rivoire, Kelley and Conerly, Thomas and Olah, Chris and Batson, Joshua},
  year = {2025},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2025/attribution-graphs/biology.html},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/On the Biology of a Large Language Model.pdf}
}

@misc{liuBackstoryBackpropagation2023,
  title = {The {{Backstory}} of {{Backpropagation}}},
  author = {Liu, Yuxi},
  year = {2023},
  month = dec,
  journal = {Yuxi on the Wired},
  url = {https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/},
  urldate = {2024-11-26},
  abstract = {Why backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.},
  langid = {english}
}

@book{mackayInformationTheoryInference2003,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = {2003},
  edition = {22nd printing},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  isbn = {978-0-521-64298-9},
  langid = {english}
}

@book{mandelbrotMisbehaviorMarketsFractal2008,
  title = {The (Mis)Behavior of Markets: A Fractal View of Risk, Ruin, and Reward},
  shorttitle = {The (Mis)Behavior of Markets},
  author = {Mandelbrot, Beno{\^i}t B. and Hudson, Richard L.},
  year = {2008},
  edition = {Nouvelle edition},
  publisher = {Basic Books},
  address = {New York},
  isbn = {978-1-84668-262-9},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/The Misbehavior of Markets_ A Fractal View of Financial Turbulence - Benoit Mandelbrot.pdf}
}

@misc{marksAuditingLanguageModels2025,
  title = {Auditing Language Models for Hidden Objectives},
  author = {Marks, Samuel and Treutlein, Johannes and Bricken, Trenton and Lindsey, Jack and Marcus, Jonathan and {Mishra-Sharma}, Siddharth and Ziegler, Daniel and Ameisen, Emmanuel and Batson, Joshua and Belonax, Tim and Bowman, Samuel R. and Carter, Shan and Chen, Brian and Cunningham, Hoagy and Denison, Carson and Dietz, Florian and Golechha, Satvik and Khan, Akbir and Kirchner, Jan and Leike, Jan and Meek, Austin and {Nishimura-Gasparian}, Kei and Ong, Euan and Olah, Christopher and Pearce, Adam and Roger, Fabien and Salle, Jeanne and Shih, Andy and Tong, Meg and Thomas, Drake and Rivoire, Kelley and Jermyn, Adam and MacDiarmid, Monte and Henighan, Tom and Hubinger, Evan},
  year = {2025},
  month = mar,
  number = {arXiv:2503.10965},
  eprint = {2503.10965},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.10965},
  url = {http://arxiv.org/abs/2503.10965},
  urldate = {2025-04-25},
  abstract = {We study the feasibility of conducting alignment audits: investigations into whether models have undesired objectives. As a testbed, we train a language model with a hidden objective. Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors. We verify via out-of-distribution evaluations that the model generalizes to exhibit whatever behaviors it believes RMs rate highly, including ones not reinforced during training. We leverage this model to study alignment audits in two ways. First, we conduct a blind auditing game where four teams, unaware of the model's hidden objective or training, investigate it for concerning behaviors and their causes. Three teams successfully uncovered the model's hidden objective using techniques including interpretability with sparse autoencoders (SAEs), behavioral attacks, and training data analysis. Second, we conduct an unblinded follow-up study of eight techniques for auditing the model, analyzing their strengths and limitations. Overall, our work provides a concrete example of using alignment audits to discover a model's hidden objective and proposes a methodology for practicing and validating progress in alignment auditing.},
  archiveprefix = {arXiv},
  file = {/Users/vikas/Zotero/storage/7W9AAA42/Marks et al. - 2025 - Auditing language models for hidden objectives.pdf;/Users/vikas/Zotero/storage/KDISMXVR/2503.html}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  journal = {The Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {0007-4985, 1522-9602},
  doi = {10.1007/BF02478259},
  url = {https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf},
  urldate = {2024-11-09},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/McCulloch.and.Pitts-1943.pdf}
}

@book{mechnerReplayMemoirUprooted2024,
  title = {Replay: Memoir of an Uprooted Family},
  shorttitle = {Replay},
  author = {Mechner, Jordan},
  year = {2024},
  edition = {First edition},
  publisher = {First Second},
  address = {New York, NY},
  abstract = {"In this intergenerational graphic memoir, renowned video game designer Jordan Mechner traces his family's journey through war, Nazi occupation, and everyday marital strife."--Amazon},
  isbn = {978-1-250-87375-0},
  langid = {english}
}

@article{meijerMakingMoneyUsing2017,
  title = {Making {{Money Using Math}}: {{Modern}} Applications Are Increasingly Using Probabilistic Machine-Learned Models.},
  shorttitle = {Making {{Money Using Math}}},
  author = {Meijer, Erik},
  year = {2017},
  month = feb,
  journal = {Queue},
  volume = {15},
  number = {1},
  pages = {16--37},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/3055301.3055303},
  url = {https://dl.acm.org/doi/10.1145/3055301.3055303},
  urldate = {2024-11-29},
  abstract = {A big difference between human-written code and learned models is that the latter are usually not represented by text and hence are not understandable by human developers or manipulable by existing tools. The consequence is that none of the traditional software engineering techniques for conventional programs (such as code reviews, source control, and debugging) are applicable anymore. Since incomprehensibility is not unique to learned code, these aspects are not of concern here.},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Meijer - 2017 - Making Money Using Math Modern applications are increasingly using probabilistic machine-learned mo.pdf}
}

@misc{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  shorttitle = {Word2vec},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1310.4546},
  url = {https://arxiv.org/abs/1310.4546},
  urldate = {2024-10-12},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{mishraAttentionCognitionCurrent2025,
  title = {Attention and Cognition: Current Issues and Recent Approaches},
  shorttitle = {Attention and Cognition},
  editor = {Mishra, Ramesh Kumar and Prasad, Seema},
  year = {2025},
  publisher = {Routledge},
  address = {Abingdon, Oxon ; New York, NY},
  abstract = {"This volume explores the psychology of attention and discusses the various cognitive processes that determine what we pay attention to. Much of our conscious moments are spent interacting with visual objects that we attend to and integrate into our consciousness. This book examines how we navigate this complex world of real and virtual objects and provides a comprehensive overview of the concept of attention and cognition. It focuses on themes related to different aspects of attention such as visual cognition and spatial orienting, selection history, priority map, the role of consciousness in attention research, and distractor suppression. It also discusses issues related to forming a comprehensive taxonomy of attention, emerging trends in attention research today and how they are being consolidated into the existing knowledge. Based on empirical research, this book will be of interest to students, researchers and teachers of cognitive psychology, cognitive science, cognitive and computational neuroscience, and computer sciences. It will also be useful to academicians, psychologists, neuroscientists, mental health professionals and counsellors"--},
  isbn = {978-1-003-36089-6},
  lccn = {BF321},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Attention and Cognition - Seema.pdf}
}

@book{mukherjeaCoffeeCanInvesting2018,
  title = {Coffee Can Investing: The Low-Risk Road to Stupendous Wealth},
  shorttitle = {Coffee Can Investing},
  author = {Mukherjea, Saurabh and Ranjan, Rakshit and Uniyal, Pranab},
  year = {2018},
  publisher = {Portfolio/Penguin},
  address = {Gurgaon, Haryana, India},
  isbn = {978-0-670-09045-7},
  langid = {english}
}

@book{Munger_2024,
  title = {The {{YouTube}} Apparatus},
  author = {Munger, Kevin},
  year = {2024},
  series = {Elements in Politics and Communication},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/the-youtube-apparatus.pdf}
}

@article{naikComprehensiveReviewComputer2022,
  title = {A {{Comprehensive Review}} of {{Computer Vision}} in {{Sports}}: {{Open Issues}}, {{Future Trends}} and {{Research Directions}}},
  shorttitle = {A {{Comprehensive Review}} of {{Computer Vision}} in {{Sports}}},
  author = {Naik, Banoth Thulasya and Hashmi, Mohammad Farukh and Bokde, Neeraj Dhanraj},
  year = {2022},
  month = apr,
  journal = {Applied Sciences},
  volume = {12},
  number = {9},
  pages = {4429},
  issn = {2076-3417},
  doi = {10.3390/app12094429},
  url = {https://www.mdpi.com/2076-3417/12/9/4429},
  urldate = {2025-02-23},
  abstract = {Recent developments in video analysis of sports and computer vision techniques have achieved significant improvements to enable a variety of critical operations. To provide enhanced information, such as detailed complex analysis in sports such as soccer, basketball, cricket, and badminton, studies have focused mainly on computer vision techniques employed to carry out different tasks. This paper presents a comprehensive review of sports video analysis for various applications: high-level analysis such as detection and classification of players, tracking players or balls in sports and predicting the trajectories of players or balls, recognizing the team's strategies, and classifying various events in sports. The paper further discusses published works in a variety of application-specific tasks related to sports and the present researcher's views regarding them. Since there is a wide research scope in sports for deploying computer vision techniques in various sports, some of the publicly available datasets related to a particular sport have been discussed. This paper reviews detailed discussion on some of the artificial intelligence (AI) applications, GPU-based work-stations and embedded platforms in sports vision. Finally, this review identifies the research directions, probable challenges, and future trends in the area of visual recognition in sports.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/A Comprehensive Review of Computer Vision in Sports Open Issues, Future Trends and Research Directions.pdf}
}

@techreport{nationalstatisticsofficeHouseholdConsumptionExpenditure2024,
  title = {Household {{Consumption Expenditure Survey}} 2023-24},
  author = {{National Statistics Office}},
  year = {2024},
  month = dec,
  institution = {National Statistics Office},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/HCES FactSheet 2023-24.pdf}
}

@misc{neilsenVisualProofThat2019,
  title = {A Visual Proof That Neural Nets Can Compute Any Function},
  author = {Neilsen, Michael},
  year = {2019},
  url = {http://neuralnetworksanddeeplearning.com/chap4.html}
}

@book{nilssonQuestArtificialIntelligence2010,
  title = {The Quest for Artificial Intelligence: A History of Ideas and Achievements},
  shorttitle = {The Quest for Artificial Intelligence},
  author = {Nilsson, Nils J.},
  year = {2010},
  publisher = {Cambridge University Press},
  address = {Cambridge ; New York},
  isbn = {978-0-521-11639-8 978-0-521-12293-1},
  lccn = {Q335 .N55 2010},
  annotation = {OCLC: ocn396185265}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02155},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2025-03-02},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Training language models to follow instructions with human feedback.pdf}
}

@misc{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1912.01703},
  url = {https://arxiv.org/abs/1912.01703},
  urldate = {2024-10-11},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{patrickmckenzieAnatomyCreditCard2024,
  title = {Anatomy of a Credit Card Rewards Program},
  author = {{Patrick McKenzie}},
  year = {2024},
  month = mar,
  url = {https://www.bitsaboutmoney.com/archive/anatomy-of-credit-card-rewards-programs/},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Anatomy of a credit card rewards program.pdf}
}

@article{peternaurProgrammingTheoryBuilding1985,
  title = {Programming as Theory Building},
  author = {{Peter Naur}},
  year = {1985},
  month = may,
  journal = {Microprocessing and Microprogramming},
  volume = {15},
  number = {5},
  pages = {253--261},
  issn = {01656074},
  doi = {10.1016/0165-6074(85)90032-8},
  url = {https://pages.cs.wisc.edu/~remzi/Naur.pdf},
  urldate = {2024-10-09},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {to-note},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Naur - Programming as Theory Building.pdf}
}

@book{princeUnderstandingDeepLearning2024,
  title = {Understanding {{Deep Learning}}},
  author = {Prince, Simon J D},
  year = {2024},
  month = oct,
  publisher = {MIT Press},
  url = {https://udlbook.github.io/udlbook/},
  isbn = {978-0-262-04864-4},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/UnderstandingDeepLearning_01_10_24_C.pdf}
}

@misc{richpangAlternativeConstructionShannon2024,
  title = {An Alternative Construction of {{Shannon}} Entropy -- {{Rich Pang}}},
  author = {{Rich Pang}},
  year = {2024},
  month = aug,
  url = {https://rkp.science/an-alternative-construction-of-shannon-entropy/},
  urldate = {2024-11-18},
  langid = {american},
  file = {/Users/vikas/Zotero/storage/ISZYJ7ST/an-alternative-construction-of-shannon-entropy.html}
}

@misc{richsuttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {{Rich Sutton}},
  year = {2019},
  month = mar,
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}

@incollection{rumelhartChapter8Learning1986,
  title = {Chapter 8 - {{Learning Internal Representations}} by {{Error Propagation}}},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition; v. 1: {{Foundations}}},
  author = {Rumelhart, David E. and Hinton, Geoffrey E},
  year = {1986},
  series = {Computational Models of Cognition and Perception},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  abstract = {What makes people smarter than computers? These volumes by a pioneering neurocomputing group suggest that the answer lies in the massively parallel architecture of the human mind. They describe a new theory of cognition called connectionism that is challenging the idea of symbolic computation that has traditionally been at the center of debate in theoretical discussions about the mind. The authors' theory assumes the mind is composed of a great number of elementary units connected in a neural network. Mental processes are interactions between these units which excite and inhibit each other in parallel rather than sequential operations. In this context, knowledge can no longer be thought of as stored in localized structures; instead, it consists of the connections between pairs of units that are distributed throughout the network. Volume 1 lays the foundations of this exciting theory of parallel distributed processing, while Volume 2 applies it to a number of specific issues in cognitive science and neuroscience, with chapters describing models of aspects of perception, memory, language, and thought},
  collaborator = {McClelland, James L. and {University of California, San Diego}},
  isbn = {978-0-262-29140-8},
  langid = {english}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2024-10-18},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@book{russellArtificialIntelligenceModern2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2021},
  series = {Pearson {{Series}} in {{Artificial Intelligence}}},
  edition = {Fourth Edition},
  publisher = {Pearson},
  address = {Hoboken, NJ},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  collaborator = {Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikash and Pearl, Judea and Wooldridge, Michael J.},
  isbn = {978-0-13-461099-3},
  langid = {english}
}

@misc{scardapane2024alicesadventuresdifferentiablewonderland,
  title = {Alice's Adventures in a Differentiable Wonderland -- Volume {{I}}, a Tour of the Land},
  author = {Scardapane, Simone},
  year = {2024},
  eprint = {2404.17625},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2404.17625},
  archiveprefix = {arXiv},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Alice Adventures in Differentiable Land.pdf}
}

@misc{scottwernerComingKnowledgeWorkSupplyChain2025,
  type = {Substack},
  title = {The {{Coming Knowledge-Work Supply-Chain Crisis}}},
  author = {{Scott Werner}},
  year = {2025},
  month = apr,
  journal = {Works on My Machine},
  url = {https://substack.com/home/post/p-162257174},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/The Coming Knowledge-Work Supply-Chain Crisis.pdf}
}

@book{sethklarmanMarginSafety1991,
  title = {Margin of {{Safety}}},
  author = {{Seth Klarman}},
  year = {1991},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Seth Klarman - Margin of Safety.pdf}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude},
  year = {1948},
  journal = {Bell System Technical Journal},
  volume = {27},
  url = {https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf}
}

@article{shannonPredictionEntropyPrinted1951,
  title = {Prediction and {{Entropy}} of {{Printed English}}},
  author = {Shannon, Claude},
  year = {1951},
  month = jan,
  journal = {Bell System Technical Journal},
  volume = {30},
  number = {1},
  pages = {50--64},
  issn = {00058580},
  doi = {10.1002/j.1538-7305.1951.tb01366.x},
  url = {https://ieeexplore.ieee.org/document/6773263},
  urldate = {2024-10-18},
  langid = {english}
}

@misc{sinnLandauChessTournaments2022,
  title = {Landau on {{Chess Tournaments}} and {{Google}}'s {{PageRank}}},
  author = {Sinn, Rainer and Ziegler, G{\"u}nter M.},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.17300},
  url = {https://arxiv.org/abs/2210.17300},
  urldate = {2024-11-02},
  abstract = {In his first mathematical paper, published in 1895 when he was 18, Edmund Landau suggested a new way to determine the winner of a chess tournament by not simply adding for each player the fixed number of points they would get for each win or draw, but rather by considering the performance of all players in the tournament relative to each other: each player would get more credit for games won or drawn against stronger players. Landau called this "relative Wertbemessung", which translates to relative score. The basic idea from linear algebra behind this scoring system was rediscovered and reused in many contexts since 1895; in particular, it is a central ingredient in Google's PageRank.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Landau on Chess Tournaments and Google's PageRank.pdf}
}

@book{sontagRegardingPainOthers2003,
  title = {Regarding the Pain of Others},
  author = {Sontag, Susan},
  year = {2003},
  edition = {1. ed},
  publisher = {{Farrar, Straus and Giroux}},
  address = {New York, NY},
  isbn = {978-0-374-24858-1},
  langid = {english}
}

@book{strogatzInfinitePowersStory2019,
  title = {Infinite Powers: The Story of Calculus ; the Language of the Universe},
  shorttitle = {Infinite Powers},
  author = {Strogatz, Steven H.},
  year = {2019},
  publisher = {Atlantic Books},
  address = {London},
  abstract = {This is the captivating story of mathematics' greatest ever idea: calculus. Without it, there would be no computers, no microwave ovens, no GPS, and no space travel. But before it gave modern man almost infinite powers, calculus was behind centuries of controversy, competition, and even death.00Taking us on a thrilling journey through three millennia, professor Steven Strogatz charts the development of this seminal achievement from the days of Aristotle to today's million-dollar reward that awaits whoever cracks Reimann's hypothesis. Filled with idiosyncratic characters from Pythagoras to Euler, Infinite Powers is a compelling human drama that reveals the legacy of calculus on nearly every aspect of modern civilisation, including science, politics, ethics, philosophy, and much besides},
  isbn = {978-1-78649-295-1 978-1-78649-294-4},
  lccn = {QA303.2 .S78 2019},
  annotation = {OCLC: on1085163529}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  lccn = {Q325.6 .R45 2018},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/RL An Introduction Sutton.pdf}
}

@book{terrencej.sejnowskiDeepLearningRevolution2018,
  title = {The Deep Learning Revolution},
  author = {{Terrence J. Sejnowski}},
  year = {2018},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-03803-4},
  lccn = {Q325.5 .S45 2018},
  file = {/Users/vikasprasad/Downloads/The Deep Learning Revolution - Sejnowski.pdf}
}

@misc{thevaticanANTIQUANOVANote2025,
  title = {{{ANTIQUA ET NOVA}}, {{Note}} on the {{Relationship Between Artificial Intelligence}} and {{Human Intelligence}}},
  shorttitle = {{{ANTIQUA ET NOVA}}},
  author = {{The Vatican}},
  year = {2025},
  month = jan,
  url = {https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html}
}

@article{tomwolfeTinkeringsRobertNoyce1983,
  title = {The {{Tinkerings}} of {{Robert Noyce}}},
  author = {{Tom Wolfe}},
  year = {1983},
  month = dec,
  journal = {Esquire Magazine},
  number = {December 1983},
  url = {https://web.stanford.edu/class/e145/2007_fall/materials/noyce.html},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Tom Wolfe - The Tinkerings of Robert Noyce.pdf}
}

@misc{touvronLlama2Open2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.09288},
  url = {https://arxiv.org/abs/2307.09288},
  urldate = {2024-10-18},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ucberkeleyEECS20NSignalsSystems2011,
  title = {{{EECS20N}}: {{Signals}} and {{Systems}} - {{Modem Negotiation}}},
  author = {{UC Berkeley}},
  year = {2011},
  url = {https://ptolemy.berkeley.edu/eecs20/week14/negotiation.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  urldate = {2024-10-18},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{vaughntanAIsMeaningmakingProblem2024,
  title = {{{AI}}'s Meaning-Making Problem.},
  author = {{VAUGHN TAN}},
  year = {2024},
  month = may,
  journal = {The Uncertainty Mindset},
  url = {https://uncertaintymindset.substack.com/p/ai-meaningmaking},
  urldate = {2025-04-28},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/AI_s meaning-making problem - Vaughn Tan.pdf}
}

@article{viswanathanLargeLanguageModels2024,
  title = {Large {{Language Models Enable Few-Shot Clustering}}},
  author = {Viswanathan, Vijay and Gashteovski, Kiril and Gashteovski, Kiril and Lawrence, Carolin and Wu, Tongshuang and Neubig, Graham},
  year = {2024},
  month = apr,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  pages = {321--333},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00648},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00648/120476/Large-Language-Models-Enable-Few-Shot-Clustering},
  urldate = {2025-05-28},
  abstract = {Abstract             Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.1},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Large Language Models Enable Few-Shot Clustering.pdf}
}

@article{yanaiRenaissanceMinds21st2020,
  title = {Renaissance Minds in 21st Century Science},
  author = {Yanai, Itai and Lercher, Martin},
  year = {2020},
  month = dec,
  journal = {Genome Biology},
  volume = {21},
  number = {1},
  pages = {67, s13059-020-01985-6},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-01985-6},
  url = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-01985-6},
  urldate = {2024-11-03},
  langid = {english},
  file = {/Users/vikas/Library/Mobile Documents/com~apple~CloudDocs/Zotero Library/Renaissance Minds in 21st Century.pdf}
}

@book{zevinStoriedLifeAJ2014,
  title = {The Storied Life of {{A}}.{{J}}. {{Fikry}}},
  shorttitle = {{{BOOK CLUB SET}}},
  author = {Zevin, Gabrielle},
  year = {2014},
  edition = {1st pbk. ed},
  publisher = {Algonquin Books of Chapel Hill},
  address = {Chapel Hill, North Carolina},
  abstract = {When his most prized possession, a rare collection of Poe poems, is stolen, bookstore owner A.J. Fikry begins isolating himself from his friends, family and associates before receiving a mysterious package that compels him to remake his life. The irascible A.J. Fikry, owner of Island Books, the only bookstore on Alice Island, has already lost his wife. Now, a rare book, has been stolen from right under his nose in the most embarrassing of circumstances. The store itself, it seems, will be next to go. One night upon closing, he discovers a toddler in his children's section with a note from her mother pinned to her Elmo doll: "I want Maya to grow up in a place with books and among people who care about such kinds of things. I love her very much, but I can no longer take care of her." A search for Maya's mother, A.J.'s rare book, and good childcare advice ensues, but it doesn't take long for the locals to notice the transformation of both bookstore and owner, something of particular interest to the lovely yet eccentric Knightley Press sales rep, Amelia Loman, who makes the arduous journey to Alice Island thrice each year to pitch her books to the cranky owner},
  isbn = {978-1-61620-451-8},
  langid = {english},
  annotation = {OCLC: 1343727362}
}

@book{zuckermanManWhoSolved2019,
  title = {The Man Who Solved the Market},
  author = {Zuckerman, Gregory},
  year = {2019},
  publisher = {Portfolio / Penguin},
  address = {New York, NY},
  abstract = {Jim Simons is the greatest money maker in modern financial history. No other investor--Warren Buffett, Peter Lynch, Ray Dalio, Steve Cohen, or George Soros--can touch his record. Since 1988, Renaissance's signature Medallion fund has generated average annual returns of 66 percent. The firm has earned profits of more than \$100 billion; Simons is worth twenty-three billion dollars. Drawing on unprecedented access to Simons and dozens of current and former employees, Zuckerman, a veteran Wall Street Journal investigative reporter, tells the gripping story of how a world-class mathematician and former code breaker mastered the market. Simons pioneered a data-driven, algorithmic approach that's sweeping the world. As Renaissance became a market force, its executives began influencing the world beyond finance. Simons became a major figure in scientific research, education, and liberal politics. Senior executive Robert Mercer is more responsible than anyone else for the Trump presidency, placing Steve Bannon in the campaign and funding Trump's victorious 2016 effort. Mercer also impacted the campaign behind Brexit. The Man Who Solved the Market is a portrait of a modern-day Midas who remade markets in his own image, but failed to anticipate how his success would impact his firm and his country. It's also a story of what Simons's revolution means for the rest of us. -- Provided by publisher},
  isbn = {978-0-7352-1798-0},
  langid = {english},
  annotation = {OCLC: 1115009015}
}
