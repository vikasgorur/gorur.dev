
@article{hang_li_language_2022,
	title = {Language models: past, present, and future},
	volume = {65},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Language models},
	url = {https://dl.acm.org/doi/10.1145/3490443},
	doi = {10.1145/3490443},
	abstract = {A language modeling overview, highlighting basic concepts, intuitive explanations, technical achievements, and fundamental challenges.},
	language = {en},
	number = {7},
	urldate = {2024-10-07},
	journal = {Communications of the ACM},
	author = {{Hang Li}},
	month = jul,
	year = {2022},
	keywords = {note-published},
	pages = {56--63},
	file = {Hang Li 2022 - Language Models Past Present:/Users/vikasprasad/Downloads/Hang Li 2022 - Language Models Past Present.pdf:application/pdf},
}

@book{kevin_p_murphy_probabilistic_2023,
	title = {Probabilistic {Machine} {Learning}: {Advanced} {Topics}},
	url = {http://probml.github.io/book2},
	publisher = {MIT Press},
	author = {{Kevin P. Murphy}},
	year = {2023},
	file = {Probabilistic Machine Learning - Advanced Topics:/Users/vikasprasad/Downloads/Probabilistic Machine Learning - Advanced Topics.pdf:application/pdf},
}

@book{kevin_p_murphy_probabilistic_2022,
	title = {Probabilistic {Machine} {Learning}: {An} introduction},
	url = {probml.ai},
	publisher = {MIT Press},
	author = {{Kevin P. Murphy}},
	year = {2022},
	file = {Probabilistic Machine Learning - An Introduction:/Users/vikasprasad/Downloads/Probabilistic Machine Learning - An Introduction.pdf:application/pdf},
}

@book{terrence_j_sejnowski_deep_2018,
	address = {Cambridge, Massachusetts},
	title = {The deep learning revolution},
	isbn = {978-0-262-03803-4},
	publisher = {The MIT Press},
	author = {{Terrence J. Sejnowski}},
	year = {2018},
	file = {The Deep Learning Revolution - Sejnowski:/Users/vikasprasad/Downloads/The Deep Learning Revolution - Sejnowski.pdf:application/pdf},
}

@misc{rich_sutton_bitter_2019,
	title = {The {Bitter} {Lesson}},
	url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
	author = {{Rich Sutton}},
	month = mar,
	year = {2019},
}

@misc{andrej_karpathy_deep_2022,
	title = {Deep {Neural} {Nets}: 33 years ago and 33 years from now},
	url = {https://karpathy.github.io/2022/03/14/lecun1989/},
	author = {{Andrej Karpathy}},
	month = mar,
	year = {2022},
}

@article{peter_naur_programming_1985,
	title = {Programming as theory building},
	volume = {15},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01656074},
	doi = {10.1016/0165-6074(85)90032-8},
	language = {en},
	number = {5},
	urldate = {2024-10-09},
	journal = {Microprocessing and Microprogramming},
	author = {{Peter Naur}},
	month = may,
	year = {1985},
	keywords = {to-note},
	pages = {253--261},
	file = {Naur - Programming as Theory Building:/Users/vikasprasad/Downloads/Naur - Programming as Theory Building.pdf:application/pdf},
}

@article{baydin_automatic_2015,
	title = {Automatic differentiation in machine learning: a survey},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Automatic differentiation in machine learning},
	url = {https://arxiv.org/abs/1502.05767},
	doi = {10.48550/ARXIV.1502.05767},
	abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
	urldate = {2024-10-11},
	author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	year = {2015},
	note = {Publisher: arXiv
Version Number: 4},
	keywords = {68W30, 65D25, 68T05, FOS: Computer and information sciences, G.1.4; I.2.6, Machine Learning (cs.LG), Machine Learning (stat.ML), Symbolic Computation (cs.SC)},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{PyTorch}},
	url = {https://arxiv.org/abs/1912.01703},
	doi = {10.48550/ARXIV.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), Mathematical Software (cs.MS)},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
}

@misc{andrej_karpathy_unreasonable_2015,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	author = {{Andrej Karpathy}},
	month = may,
	year = {2015},
}

@article{bengio_neural_2003,
	title = {A neural probabilistic language model},
	volume = {3},
	issn = {1532-4435},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Janvin, Christian},
	month = mar,
	year = {2003},
	note = {Publisher: JMLR.org},
	pages = {1137--1155},
}

@misc{andrej_karpathy_makemore_2022,
	title = {makemore},
	copyright = {MIT License},
	url = {https://github.com/karpathy/makemore},
	author = {{Andrej Karpathy}},
	month = jun,
	year = {2022},
}

@misc{andrej_karpathy_spelled-out_2022,
	title = {The spelled-out intro to language modeling: building makemore},
	shorttitle = {makemore 1},
	url = {https://www.youtube.com/watch?v=PaCmpygFfXo},
	author = {{Andrej Karpathy}},
	month = sep,
	year = {2022},
}

@misc{andrej_karpathy_building_2022,
	title = {Building makemore {Part} 2: {MLP}},
	shorttitle = {makemore 2},
	url = {https://youtu.be/TCH_1BHY58I?si=TgPdPxLeDGhPoSDR},
	author = {{Andrej Karpathy}},
	month = sep,
	year = {2022},
}

@misc{andrej_karpathy_building_2022-1,
	title = {Building makemore {Part} 3: {Activations} \& {Gradients}, {BatchNorm}},
	shorttitle = {makemore 3},
	url = {https://youtu.be/P6sfmUTpUmc?si=muQJymzcR2VoCHK8},
	author = {{Andrej Karpathy}},
	month = oct,
	year = {2022},
}

@misc{andrej_karpathy_building_2022-2,
	title = {Building makemore {Part} 4: {Becoming} a {Backprop} {Ninja}},
	shorttitle = {makemore 4},
	url = {https://youtu.be/q8SA3rM6ckI?si=ho-QbwoS_qyilgzO},
	author = {{Andrej Karpathy}},
	month = oct,
	year = {2022},
}

@misc{andrej_karpathy_building_2022-3,
	title = {Building makemore {Part} 5: {Building} a {WaveNet}},
	shorttitle = {makemore 5},
	url = {https://youtu.be/t3YJ5hKiMQ0?si=_eH_V25gXIkiwqgn},
	author = {{Andrej Karpathy}},
	month = nov,
	year = {2022},
}

@article{hayes_first_2013,
	title = {First {Links} in the {Markov} {Chain}},
	volume = {101},
	url = {https://doi.org/10.1511/2013.101.92},
	number = {2},
	journal = {American Scientist},
	author = {Hayes, Brian},
	year = {2013},
}

@article{shannon_mathematical_1948,
	title = {A {Mathematical} {Theory} of {Communication}},
	volume = {27},
	url = {https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf},
	journal = {Bell System Technical Journal},
	author = {Shannon, Claude E.},
	year = {1948},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	language = {en},
	number = {4},
	urldate = {2024-10-11},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
}
