<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Why Machines Learn</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7fccaa5a4effadb8a2e8cfc8e0465904.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Why Machines Learn</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>Last updated</em>: Nov 27, 2024.</p>
<p>These are interesting things mentioned in <em>Why Machines Learn: The Elegant Maths behind Modern AI</em>, <span class="citation" data-cites="ananthaswamyWhyMachinesLearn2024">Ananthaswamy (<a href="#ref-ananthaswamyWhyMachinesLearn2024" role="doc-biblioref">2024</a>)</span>.</p>
<p><strong>Imprinting</strong>: Konrad Lorenz discovered that ducklings imprint on the first moving thing they see after hatching. More interestingly, they can imprint on <em>relations</em>. If upon birth they see two moving red objects, they will later follow two objects of the same color, even if the color is different. More about this in his Nobel lecture <span class="citation" data-cites="konradlorenzAnalogySourceKnowledge1973">(<a href="#ref-konradlorenzAnalogySourceKnowledge1973" role="doc-biblioref">Konrad Lorenz 1973a</a>)</span> and biography <span class="citation" data-cites="konradlorenzKonradLorenzBiography1973">(<a href="#ref-konradlorenzKonradLorenzBiography1973" role="doc-biblioref">Konrad Lorenz 1973b</a>)</span>.</p>
<p><strong>The first artificial neuron</strong>: The paper about the first artificial neuron was a collaboration between McCulloch, a professor in his mid-40s and Pitts, a teenage prodigy who was hanging around a university and was adopted into the McCulloch home. The paper itself <span class="citation" data-cites="mccullochLogicalCalculusIdeas1943">(<a href="#ref-mccullochLogicalCalculusIdeas1943" role="doc-biblioref">McCulloch and Pitts 1943</a>)</span> is impenetrable, written in a formal math style reminiscient of <em>Principia Mathematica</em>. The important conclusion though is that combinations of the artificial neuron can implement any boolean logic.</p>
<p><strong>Hebbian learning</strong> can be understood as the memorable phrase ‚Äúneurons that fire together, wire together‚Äù.</p>
<p><strong>The Mark I perceptron</strong> was a hardware implementation that could recognize handwritten characters from a 20x20 image. It was a 3-layer neural network, although only one layer had adjustable weights (in hardware, using DC motors to drive potentiometers, essentially volume knobs!). The operator‚Äôs manual <span class="citation" data-cites="Hay1960MarkIP">(<a href="#ref-Hay1960MarkIP" role="doc-biblioref">Hay, Lynch, and Smith 1960</a>)</span> has all the fascinating details.</p>
<p>William Rowan Hamilton discovered <strong>quaternions</strong> and etched it on a bridge in Dublin. He‚Äôs also responsible for notions of ‚Äúscalar‚Äù and ‚Äúvector‚Äù.</p>
<p>A hyperplane, such as the one learnt by a perceptron, can be uniquely described by a vector that is orthogonal to it. This is in fact the vector of weights, <span class="math inline">\(w\)</span>.</p>
<p>The <strong>perceptron learning rule</strong> is simple, but it‚Äôs remarkable that it always converges if the dataset is linearly separable. The lecture notes <span class="citation" data-cites="kilianweinbergerCS457802024">(<a href="#ref-kilianweinbergerCS457802024" role="doc-biblioref">Kilian Weinberger 2024</a>)</span> have an accessible proof of this theorem, also reproduced in the book.</p>
<p>When two dialup modems are trying to establish a connection they send a pre-agreed signal that lets them configure (learn) an <strong>adaptive filter</strong> to filter out the noise particular to that line. This is some of the weird sounds we used to hear! The course notes in <span class="citation" data-cites="ucberkeleyEECS20NSignalsSystems2011">(<a href="#ref-ucberkeleyEECS20NSignalsSystems2011" role="doc-biblioref">UC Berkeley 2011</a>)</span> have more details.</p>
<p>Adaptive filters use the mean-squared error (MSE) as the cost function. When reading about linear regression I‚Äôve always wondered why we can‚Äôt just use the absolute difference. One reason to prefer MSE is that it‚Äôs differentiable everywhere. Another reason mentioned in this book is that the MSE punishes outliers much more than the absolute difference.</p>
<p>The idea of stochastic gradient descent was already invented by the <strong>ADALINE</strong> project at Stanford in the 60s, which tried to solve some of the same problems as the perceptron machine.</p>
<p>Paul Erd≈ës wasn‚Äôt convinced at first that switching doors in the Monty Hall problem was the right solution. There is hope for all of us!</p>
<p>Bayesian statistics was used by Frederick Mosteller and David Wallace in the 1940s to figure out the authorship of <em>The Federalist Papers</em>.</p>
<p>A concise way to remember Principal Component Analysis (PCA): <em>The eigenvectors of a covariance matrix are the principal components of the original data matrix.</em></p>
<p>The <strong>support vector machine</strong> (SVM) overcomes the linear separability limitation of the perceptron. It finds an <em>optimal</em> separating hyperplane by projecting the dataset to a much higher dimension and finding a plane there. The algorithm to find this hyperplane works by minimizing a cost function related to the weight vector while simultaneously satisfying a set of constraints, one per data point.</p>
<p>Constrained optimization required by SVMs uses something called the technique of <strong>Lagrange multipliers</strong>. It consists of defining a new function, the Lagrange function, that encodes all the constraints and then finding the extrema of it.</p>
<p>The optimal separating hyperplane of the SVM depends only on the dot produces of a few ‚Äúsupport vectors‚Äù that anchor the margin, hence the name. However, computing the dot products in higher dimensions can be expensive (this is not very convincing to me ‚Äì is it still true?). The solution is to use the <strong>kernel trick</strong>.</p>
<p>A <em>kernel</em> is a function such that given two vectors <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> and a function <span class="math inline">\(\phi(x)\)</span> that transforms each vector to a higher dimension, the kernel <span class="math inline">\(K(x)\)</span> allows one to compute the dot product in the higher dimension while only working with the lower-dimension vectors:</p>
<p><span class="math display">\[
K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)
\]</span> The kernel trick was suggested by a French scientist Isabelle Guyon, working with Bernard Boser and Vladimir Vapnik. The trick apparently can work even when projecting to an <em>infinite</em> dimensional space, called a Hilbert space. The kernel in that case is called the ‚Äúradial basis function‚Äù (RBF).</p>
<p>An RBF kernel can <em>always</em> find a linearly separable hyperplane in some infinite-dimensional space. This means that SVMs are also <strong>universal function approximators</strong>, just like deep neural networks.</p>
<p>The original SVM paper is <span class="citation" data-cites="boserTrainingAlgorithmOptimal1992">(<a href="#ref-boserTrainingAlgorithmOptimal1992" role="doc-biblioref">Boser, Guyon, and Vapnik 1992</a>)</span>.</p>
<p>Glass is a disordered solid ‚Äî without an ordered crystalline structure yet not a liquid. By analogy, certain magnetic materials that have atoms or ions with randomly oriented magnetic moments (which arise due to ‚Äòspin‚Äô) are called <strong>spin glasses</strong>.</p>
<p>A simple mathematical model of spin glasses assumes that the spin of each element in a 2d or 3d array depends only on the spins of its neighbors. If such a material starts out in an arbitrary state, the spins will flip until the entire system reaches the state of lowest energy, which happens when the spins are all aligned.</p>
<p>The physicist John Hopfield (Nobel in Physics, 2024) was thinking about the problem of <strong>associative memory</strong>. How is it that when given a fragment of an image or a hint of a smell, we can recall an entire vivid memory?</p>
<p>The solution was the <strong>Hopfield network</strong>. The connections in such a network are arranged similarly to the spin glasses. For a given configuration, the weights of the neurons represent the ‚Äúmemory‚Äù of the network. If the memory puts the network into its lowest energy state, any <em>distorted</em> (noisy) version of the same memory would put the network in a higher energy state. The network however can find its way back to the lowest energy state through a dynamical process, like any physical system finding its equilibrium. Thus we can think of the Hopfield network as a system that stores a memory and can retrieve that memory when given a fragment of it.</p>
<p>Hopfield could only publish his paper <span class="citation" data-cites="hopfieldNeuralNetworksPhysical1982">(<a href="#ref-hopfieldNeuralNetworksPhysical1982" role="doc-biblioref">Hopfield 1982</a>)</span> because he was a member of the Academy of Sciences and thus had privileges to publish without peer review. ‚ÄúNo refereed journal would have accepted it‚Äù. üôÉ</p>
<p>George Cybenko proved in 1989 that a neural network with just one hidden layer and an arbitrarily large number of neurons can approximate any function. This is the <strong>universal approximation theorem</strong>.</p>
<p>The proof uses the idea that <strong>functions are vectors in an infinite dimensional space</strong>. It‚Äôs not a constructive proof but one by contradiction. It assumes that a network with a single hidden layer cannot span the entire vector space of functions and arrives at a contradiction.</p>
<p>A deterministic algorithm for updating the weights of a neural network suffers from the problem of <strong>symmetry</strong>. If the initial weights are all assigned the same value (say, 0), they will be updated by the learning rule in the same way and thus effectively become redundant. The simple way to solve this problem is to initialize the weights randomly, an idea that first occured to Rumelhart.</p>
<p>The backprop paper <span class="citation" data-cites="rumelhartLearningRepresentationsBackpropagating1986">(<a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span> was the ‚Äúlast time‚Äù it was discovered. See <span class="citation" data-cites="liuBackstoryBackpropagation2023">(<a href="#ref-liuBackstoryBackpropagation2023" role="doc-biblioref">Liu 2023</a>)</span> for a detailed history of backpropagation.</p>
<p>David Hubel and Torsten Wiesel did experiments on cats and figured out essentially that the neurons in the brain respond to certain ‚Äúfeatures‚Äù in the visual field, like edges.</p>
<p>Yann LeCun at Bell Labs in 1988 designed a neural network to recognize handwritten digits from a US Postal Service dataset (is this the origin of the famous MNIST dataset?). He wrote a compiler in Lisp that would take the architecture of a neural network and generate C code to implement it.</p>
<p>His network, LeNet, was a <strong>convolutional neural network</strong>. A 2d convolution of an image involves moving a small <em>kernel</em> (say 4x4) over the pixels of the image and generating a new pixel through some operation (say, average or max). The end result of this is a slightly smaller image. While these kernels could be handcrafted, it‚Äôs more scalable to let the neural net learn them. This is how a neural net learns which features are important.</p>
<p>In 2009, Fei-Fei Li presented an immense dataset of labeled images, ImageNet and an associated challenge. Use the 1.2 million images, binned into 1000 categories, to train an algorithm and test it on 100,000 unseen images. AlexNet, using GPUs for training won the contest in 2012 by a wide margin, kicking off the modern deep learning revolution.</p>
<p><strong>Grokking</strong> is a rather strange properly of deep neural networks. The cutting edge models today have parameters in the billions or trillions, sometimes outnumbering the instances of data used to train them. In theory these networks should simply overfit the data and not generalize, but that‚Äôs not true.</p>
<p>One hypothesis is that some kind of implicit regularization is happening in the training process. See also the illuminating short paper <span class="citation" data-cites="breimanReflectionsRefereeingPapers2018">(<a href="#ref-breimanReflectionsRefereeingPapers2018" role="doc-biblioref"><strong>breimanReflectionsRefereeingPapers2018?</strong></a>)</span> that questions the usefulness of ‚Äútheory‚Äù when doing ML research.</p>
<p>LLMs are an instance of self-supervised learning. The pre-training helps the network learn some structure of language (or images, speech, ‚Ä¶) and fine-tuning later guides them towards a purpose.</p>
<p>The theory of all this is far, far behind experimentation.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ananthaswamyWhyMachinesLearn2024" class="csl-entry" role="listitem">
Ananthaswamy, Anil. 2024. <em>Why Machines Learn: The Elegant Math Behind Modern <span>AI</span></em>. New York: Dutton.
</div>
<div id="ref-boserTrainingAlgorithmOptimal1992" class="csl-entry" role="listitem">
Boser, Bernhard E., Isabelle M. Guyon, and Vladimir N. Vapnik. 1992. <span>‚ÄúA Training Algorithm for Optimal Margin Classifiers.‚Äù</span> In <em>Proceedings of the Fifth Annual Workshop on <span>Computational</span> Learning Theory</em>, 144‚Äì52. Pittsburgh Pennsylvania USA: ACM. <a href="https://doi.org/10.1145/130385.130401">https://doi.org/10.1145/130385.130401</a>.
</div>
<div id="ref-Hay1960MarkIP" class="csl-entry" role="listitem">
Hay, John C., B. Lynch, and David Russell Bedford Smith. 1960. <span>‚ÄúMark <span>I</span> Perceptron Operators‚Äô Manual.‚Äù</span>
</div>
<div id="ref-hopfieldNeuralNetworksPhysical1982" class="csl-entry" role="listitem">
Hopfield, J J. 1982. <span>‚ÄúNeural Networks and Physical Systems with Emergent Collective Computational Abilities.‚Äù</span> <em>Proceedings of the National Academy of Sciences</em> 79 (8): 2554‚Äì58. <a href="https://doi.org/10.1073/pnas.79.8.2554">https://doi.org/10.1073/pnas.79.8.2554</a>.
</div>
<div id="ref-kilianweinbergerCS457802024" class="csl-entry" role="listitem">
Kilian Weinberger. 2024. <span>‚Äú<span>CS</span> 4/5780: <span>Intro</span> to <span>Machine Learning</span>.‚Äù</span>
</div>
<div id="ref-konradlorenzAnalogySourceKnowledge1973" class="csl-entry" role="listitem">
Konrad Lorenz. 1973a. <span>‚ÄúAnalogy as a <span>Source</span> of <span>Knowledge</span>.‚Äù</span> Stockholm, Sweden.
</div>
<div id="ref-konradlorenzKonradLorenzBiography1973" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî. 1973b. <span>‚ÄúKonrad <span>Lorenz</span>: <span>Biography</span>.‚Äù</span>
</div>
<div id="ref-liuBackstoryBackpropagation2023" class="csl-entry" role="listitem">
Liu, Yuxi. 2023. <span>‚ÄúThe <span>Backstory</span> of <span>Backpropagation</span>.‚Äù</span> <em>Yuxi on the Wired</em>. https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/.
</div>
<div id="ref-mccullochLogicalCalculusIdeas1943" class="csl-entry" role="listitem">
McCulloch, Warren S., and Walter Pitts. 1943. <span>‚ÄúA Logical Calculus of the Ideas Immanent in Nervous Activity.‚Äù</span> <em>The Bulletin of Mathematical Biophysics</em> 5 (4): 115‚Äì33. <a href="https://doi.org/10.1007/BF02478259">https://doi.org/10.1007/BF02478259</a>.
</div>
<div id="ref-rumelhartLearningRepresentationsBackpropagating1986" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>‚ÄúLearning Representations by Back-Propagating Errors.‚Äù</span> <em>Nature</em> 323 (6088): 533‚Äì36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div>
<div id="ref-ucberkeleyEECS20NSignalsSystems2011" class="csl-entry" role="listitem">
UC Berkeley. 2011. <span>‚Äú<span>EECS20N</span>: <span>Signals</span> and <span>Systems</span> - <span>Modem Negotiation</span>.‚Äù</span>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>