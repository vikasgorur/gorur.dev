[
  {
    "objectID": "annotations/history-of-language-models.html",
    "href": "annotations/history-of-language-models.html",
    "title": "History of Language Models",
    "section": "",
    "text": "This annotation is for the paper (Li 2022)\n\n\n\nLast updated: Oct 7, 2024.\n\nThere are two fundamental approaches to language modeling: one based on probability theory and the other based on language theory (grammars).\nA language model is a probability distribution defined on a word (token) sequence. More seriously, the probability of a given sequence of words \\(w_1, w_2, ..., w_N\\) is the product of successive conditional probabilities:\n\nprobability of the second word given the first\nprobability of the third word given the first and second\nand so on\n\n\\[\np(w_1, w_2, ..., w_N) = \\prod_{i=1}^{N} p(w_i | w_1, w_2, ..., w_{i-1})\n\\]\nMarkov invented the concept of Markov processes when studying language. See (Hayes 2013) for a fascinating history of how he analyzed the text of Pushkin’s Eugene Onegin by hand.\nAn \\(n\\)-gram language model assumes that the probability of a word depends only on the words at the previous \\(n - 1\\) positions. This kind of model is a Markov chain of the order \\(n - 1\\).\nShannon (1948) defined the concepts of entropy and cross-entropy. The cross-entropy is a measure of how well the model has learned the “true” probability. What is this true probability? I think it’s just the probability computed using frequencies in the training corpus.\nThe other approach to language modeling was the hierarchy of grammars proposed by Chomsky in 1956. This is not at all influential anymore. Chomsky thought that finite state grammars (like n-gram models) are limited and that context-free grammars can model language more effectively.\nYoshua Bengio (2003) (Turing Award 2018) first used neural networks for language modeling. Their paper had two key ideas:\n\nWords (tokens) have a “distributed representation” as vectors. This is the embedding.\nThe language model is a neural network.\n\nThe number of parameters of this model is of the order \\(O(V)\\) where \\(V\\) is the size of the vocabulary.\nThe next step in neural language modeling was the use of recurrent neural networks (RNNs). See Karpathy (2015) on their “unreasonable effectiveness”.\nA “conditional” language model calculates probability of word sequence under the condition of a previous sequence. These are the seq2seq models. These models can do tasks such as machine translation where the input and output are both sequences of tokens.\nSeq2seq models led to the transformer and pre-trained language models. A pre-trained model:\n\nUses a large corpus to do unsupervised learning to train the parameters.\nThe model is then fine-tuned with a small number of examples to adapt the model to a specific task.\n\nThis paper claims that language models don’t have reasoning, only association. I think that claim stopped being true mere months after this paper was published.\nIt is known that human language understanding is based on representations of concepts in many modes: visual, auditory, tactile, olfactory. Can AI language models learn from multiple modes as well?\n\n\n\n\nReferences\n\nHayes, Brian. 2013. “First Links in the Markov Chain.” American Scientist 101 (2). https://doi.org/10.1511/2013.101.92.\n\n\nKarpathy, Andrej. 2015. “The Unreasonable Effectiveness of Recurrent Neural Networks.” https://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n\n\nLi, Hang. 2022. “Language Models: Past, Present, and Future.” Communications of the ACM 65 (7). https://cacm.acm.org/research/language-models/.\n\n\nShannon, Claude E. 1948. “A Mathematical Theory of Communication.” Bell System Technical Journal 27. https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf.\n\n\nYoshua Bengio, Pascal Vincent, Rejean Ducharme. 2003. “A Neural Probabilistic Language Model.” Journal of Machine Learning Research 3 (Feb). http://www.jmlr.org/papers/v3/bengio03a.html."
  },
  {
    "objectID": "posts/counting-is-hard.html",
    "href": "posts/counting-is-hard.html",
    "title": "Counting is hard",
    "section": "",
    "text": "George Casella, Roger L. Berger, Statistical Inference, 2nd edition, 2002. Exercise 1.20.\nMy telephone rings 12 times each week, the calls being randomly distributed among the 7 days. What is the probability that I get atleast one call each day?\nLast updated: Jul 26, 2024.\nThis problem seems simple enough at first. Each of the 12 phone calls can independently happen on any of the 7 days, so the total number of ways to distribute the phone calls is \\(7^{12}\\).\nWe want each day to have atleast one phone call, so let’s first choose 7 out of 12 calls and distribute them one per day. The number of ways to do this is: \\({12 \\choose 7} \\cdot 7!\\). Each of the remaining 5 calls can happen on any of the 7 days, so the number of ways is \\(7^5\\). Putting it all together the probability we want is:\n\\[\n\\frac{{12 \\choose 7} \\cdot 7! \\cdot 7^5}{7^{12}}\n\\]\nimport math\n\nmath.comb(12, 7) * math.factorial(7) * 7**5 / 7**12\n\n4.846960025159585\nWell, that’s not right. I tried a few other ways to arrive at an answer but couldn’t be sure that any of them were correct. So I gave up and wrote code instead:\nimport fastrand\n\ndef phonecalls():\n    N = 1_000_000\n    count = 0\n    week = [0, 0, 0, 0, 0, 0, 0]\n\n    for i in range(N):\n        for _ in range(12):\n            week[fastrand.pcg32randint(0, 6)] += 1\n\n        if week.count(0) == 0:\n            count += 1\n\n        week = [0, 0, 0, 0, 0, 0, 0]\n    \n    return count / N\n\nphonecalls()\n\n0.228721\nI looked up the answer in the solutions manual for the textbook. It says:\nThis answer doesn’t feel very satisfying. Even if I’d come up with it myself, I would feel more confident about the correctness of my code than this answer. Why is it so complicated?\nI’m left with more questions about counting:"
  },
  {
    "objectID": "posts/counting-is-hard.html#a-performance-lesson",
    "href": "posts/counting-is-hard.html#a-performance-lesson",
    "title": "Counting is hard",
    "section": "A performance lesson",
    "text": "A performance lesson\nI initially wrote the code using NumPy but after benchmarking found that: - The vectors aren’t big enough in this case so the overhead of NumPy dominates. - Most of the time is spent in generating the random numbers.\nThe second point led me to discover the library fastrand and its accompanying paper. The impact of just swapping out the random number generator is below:\n\n\n\nmethod\ntime\n\n\n\n\nrandom.randint\n3460 ms\n\n\nfastrand\n770 ms"
  },
  {
    "objectID": "posts/counting-is-hard.html#further-reading",
    "href": "posts/counting-is-hard.html#further-reading",
    "title": "Counting is hard",
    "section": "Further reading",
    "text": "Further reading\n\n\n\n\n\n\n\nDaniel Lemire, Fast Random Integer Generation in an Interval, ACM Transactions on Modeling and Computer Simulation, Volume 29 Issue 1, February 2019.\n\n\n\n\nThe paper that describes the algorithm implemented by the fastrand library. Its key insight is that bounded random numbers can be generated by doing fewer expensive integer divisions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vikas Gorur",
    "section": "",
    "text": "Engineering at Airbase, previously Twitter, Blurb, Gluster.\n@vikasgorur"
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Vikas Gorur",
    "section": "Machine Learning",
    "text": "Machine Learning\nThis is my attempt to build the “tech tree” of ML, starting from the absolute basics and ending up somewhere around GPT-4.\n\nLearning by gradient descent\nBreaking Caesar ciphers using relative entropy\n\n\nmakemore"
  },
  {
    "objectID": "index.html#applied-math",
    "href": "index.html#applied-math",
    "title": "Vikas Gorur",
    "section": "Applied Math",
    "text": "Applied Math\n\nCounting is hard"
  },
  {
    "objectID": "index.html#annotations",
    "href": "index.html#annotations",
    "title": "Vikas Gorur",
    "section": "Annotations",
    "text": "Annotations\nMy notes on papers and articles I’ve read.\n\nHistory of Language Modeling"
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html",
    "href": "posts/learning-by-gradient-descent.html",
    "title": "Learning by gradient descent",
    "section": "",
    "text": "This is one of the posts in a series that aims to build an understanding of Large Language Models (LLMs) starting from the absolute basics. The only background knowledge assumed is some coding ability and pre-college math.\nLast updated: Aug 31, 2024.\nWhen we first learn programming, we learn to give the computer precise instructions to solve a problem. A program is an encoding of procedural knowledge:\n– Structure and Interpretation of Computer Programs\nMachine learning is a radically different way of using computers to solve problems. We assume that in some platonic realm there exists a function that perfectly solves our problem. We try to approximate this function with a family of functions and call it our model. We pick a specific member of that family by learning the parameters of the model using training data."
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html#problem-how-long-will-this-job-run",
    "href": "posts/learning-by-gradient-descent.html#problem-how-long-will-this-job-run",
    "title": "Learning by gradient descent",
    "section": "Problem: how long will this job run?",
    "text": "Problem: how long will this job run?\nA note about finding problems. When I used to endlessly consume ML textbooks, videos, blog posts, I always came away a bit dissatisfied and feeling like I hadn’t really learned anything. Will the method I just learned work on anything other than the Iris dataset from 1936? Learning that way skipped over one of the hard parts of doing ML: figuring out what kind of model would even work for a given problem. If you have felt the same way, I encourage you to find problems and datasets from your own life, or atleast find a different dataset on your own and try to apply your newly learned techniques to it.\nFor this post I’ve assembled a dataset from a problem I encountered myself. Assume there is an ML training job that you want to run on datasets of varying sizes. It’s not important what the job does. The only intuition we need is the reasonable expectation that the running time of the job is proportional to the number of training examples in a given run. We can scatter plot the data and confirm this intuition.\n\nimport pandas as pd\n\nmetrics = pd.read_csv(\"data/metrics.csv\")\n\n\n\n\n\n\n\n\n\n\nGiven that we have one continuous input variable n and we wish to predict another continuous variable duration, the simplest model to try is a line that is closest to all the points. For reasons of convention we’ll denote our input as the vector \\({\\textbf X}\\) and the output as the vector \\({\\textbf Y}.\\)\n(Note that we’re scaling both \\({\\textbf X}\\) and \\({\\textbf Y}\\) values to be in the range \\([0, 1]\\). This is necessary for most ML algorithms to work well, but I don’t understand it deeply enough to explain in this post.)\n\ndef scale(v):\n    return (v - v.min()) / (v.max() - v.min())\n\n\nX = scale(metrics[\"n\"].to_numpy())\nY = scale(metrics[\"duration\"].to_numpy())\n\nNow we can write our model as:\n\\[\n{\\textbf Y} = \\theta_0 + \\theta_1 {\\textbf X}\n\\]\nIn Python:\n\ndef prediction(X, θ):\n    return θ[0] + θ[1] * X\n\nNote that we’re multiplying a vector X with a scalar θ[1]. This works because NumPy supports broadcasting; see notes at the end of this post."
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html#learning-the-parameters",
    "href": "posts/learning-by-gradient-descent.html#learning-the-parameters",
    "title": "Learning by gradient descent",
    "section": "Learning the parameters",
    "text": "Learning the parameters\nThe model above describes an infinite number of lines. To find a specific line that best fits the available data, we need to find the values of \\(\\theta_0\\) and \\(\\theta_1\\). We’re also making the assumption that such a line will help us predict the output variable on unseen future data. For more complicated models this is not true by default and special measures need to be taken to reduce overfitting.\nHow can we find the parameter values? The answer is one of the most important ideas in ML:\nThe parameters of a model are found by minimizing a loss function.\nThe loss function (also called the cost or objective) is a measure of how well a model fits its training data. Thus it is a function of both the parameters of the model and the training data.\nIn our problem we want the loss value to be 0 if the prediction values (denote as \\({\\hat {\\textbf Y}}\\)) exactly match the training values \\({\\textbf Y}\\). This is not possible because our data points don’t all lie on a single line. The next best thing therefore is to find parameter values such that the loss is the lowest value possible.\nWe thus want our loss function to have the following reasonable properties:\n\nIt must measure the distance between the prediction and the correct value. If the predictions are far off, the loss needs to be higher.\nEach training example must contribute to the loss.\n\nWe can thus derive the following loss function:\n\\[\nL(\\theta, {\\textbf X}, {\\textbf Y}) = \\sum_{i = 1}^{n} (Y_i - \\theta_0 + \\theta_1 X_i)^2\n\\]\n\ndef loss(X, Y, θ):\n    return ((Y - prediction(X, θ)) ** 2).sum()\n\nWhy are we squaring each of the terms? Why aren’t we just using the absolute value of the difference? I’m sure there are many reasons for it, but one of them is that we are going to differentiate this function to find its minimum."
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html#gradient-descent",
    "href": "posts/learning-by-gradient-descent.html#gradient-descent",
    "title": "Learning by gradient descent",
    "section": "Gradient descent",
    "text": "Gradient descent\nThe problem of finding the right \\(\\theta\\) to fit a line has an exact mathematical solution, but we’re going to find it the ML way using the technique of gradient descent.\nWe start with arbitrary values for \\(\\theta\\) and iteratively change them such that the loss gets smaller. If you imagine the loss function as a 3d surface (in this case it looks like a bowl), we start somewhere on that surface and continuously try to move downhill.\nRecall that the derivative of a function is how much its value changes when its input changes by a little bit. If the derivative at a point is positive, it means a small positive change in the input causes the function to increase. If the derivative is negative, a small positive change in the input causes the function to decrease.\nThus if our goal is to minimize \\(L(\\theta)\\), we should modify each parameter at each step by an amount that’s proportional to the derivative of the loss, but negated. Since there are many parameters we want the partial derivative of the loss with respect to each parameter, and all these derivatives considered together is the gradient.\nWe can derive expressions for the gradient by normal calculus:\n\\[\\begin{eqnarray}\n\\frac{\\partial L}{\\partial \\theta_0} &=& \\sum_{i = 0}^{n} 2 ({\\hat y}_i - y_i) \\\\\n\n\\frac{\\partial L}{\\partial \\theta_1} &=& \\sum_{i = 0}^{n} 2 \\cdot ({\\hat y}_i - y_i) \\cdot x_i \\\\\n\\end{eqnarray}\\]\nIn code we’ll call this the full_gradient, since we’re using the entire dataset to compute it. Ignore the last parameter (_) for now, it’ll become relevant soon enough.\n\nimport numpy as np\n\ndef full_gradient(X, Y, θ, _):\n    return np.array([\n        np.sum(2 * (prediction(X, θ) - Y)),\n        np.sum(2 * (prediction(X, θ) - Y) * X)\n    ])\n\nThe descend function below iteratively updates the parameters based on the gradient. The key line of code is:\nθ = θ - λ * δ\nλ here is called the learning rate. It’s the size of the step the algorithm takes when descending the gradient. Picking the right value of λ is a topic on its own, but for this example I just did trial and error until I found a learning rate that works.\nThe descend function also does a couple of other things: (1) record the value of the loss periodically (2) bail out when the loss starts to converge.\n\ndef descend(gradient, λ):\n    θ = np.array([-1.0, 1.0])    # Initial values of params, picked arbitrarily.\n    iters = []\n    losses = []\n\n    l = loss(X, Y, θ)\n    for i in range(100000):\n        prev_loss = l\n        l = loss(X, Y, θ)\n\n        # Bail out if the loss has converged\n        if i &gt; 1 and abs(prev_loss - l) &lt; 1e-6:\n            iters.append(i)\n            losses.append(l)\n            break\n\n        # Record progress\n        if i == 1 or i % 100 == 0:\n            iters.append(i)\n            losses.append(l)\n\n        # Compute gradient and update params\n        δ = full_gradient(X, Y, θ, i)\n        θ = θ - λ * δ\n\n    return θ, (iters, losses)\n\nRunning the descent gives:\n\ndef run_descent(g, λ):\n    θ, (iters, losses) = descend(g, λ)\n    for i in range(len(iters)):\n        print(f\"i = {iters[i]:&lt;12} loss {losses[i]:&gt;12.8f}\")\n\n    print(f\"\\nLearned parameters: {θ}\")\n    return θ, (iters, losses)\n\nθ_full, trace_full = run_descent(full_gradient, 0.00001)\n\ni = 0            loss 785.20179039\ni = 1            loss 760.98085472\ni = 100          loss  34.67684647\ni = 200          loss   2.04681446\ni = 300          loss   0.62813506\ni = 400          loss   0.56640872\ni = 500          loss   0.56367921\ni = 564          loss   0.56354384\n\nLearned parameters: [0.00303936 1.04948881]"
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html#stochastic-gradient",
    "href": "posts/learning-by-gradient-descent.html#stochastic-gradient",
    "title": "Learning by gradient descent",
    "section": "Stochastic gradient",
    "text": "Stochastic gradient\nThere’s a massive improvement possible to the above method that sounds crazy the first time you hear it: what if instead of using the entire dataset to compute the gradient, we used just a single training example?\nThe gradient computed this way is called the stochastic gradient because it’s a random messy estimate of the true (full) gradient.\nWe implement this in code by getting rid of the loop from full_gradient and instead passing in the index (i) of the training example we want to use to compute the gradient.\n\ndef stochastic_gradient(X, Y, θ, i):\n    δ = np.zeros(2)\n    i = i % len(X)\n\n    δ[0] = 2 * (prediction(X[i], θ) - Y[i])\n    δ[1] = 2 * (prediction(X[i], θ) - Y[i]) * X[i]\n    return δ\n\nRunning the descent gives:\n\nθ_stochastic, trace_stochastic = run_descent(stochastic_gradient, 0.001)\n\ni = 0            loss 785.20179039\ni = 1            loss 242.72688292\ni = 100          loss   0.56226346\ni = 115          loss   0.56224460\n\nLearned parameters: [0.00268266 1.05993278]\n\n\nIt’s pretty close to the answer we got from using the full gradient! Note that we had to use a different learning rate (0.001) to get this to converge. We can plot both lines against the data to see how well they fit and how close they are to each other.\n\n\n\n\n\n\n\n\n\nEstimating the gradient using a fraction of the dataset makes large-scale machine learning possible. A real-world neural network like GPT-3 has 175 billion parameters, the vectors involved have dimensions in the tens of thousands, and the number of training examples is in the billions. It would be practically impossible to train a model like that by computing the full gradient on each iteration.\nThe optimization methods used to train such models are far more sophisticated (e.g., Adam) but they retain the core idea that a fuzzy estimate of the gradient derived from a subset of the data is enough to reach an acceptable minimum of the loss function."
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html#why-does-it-work",
    "href": "posts/learning-by-gradient-descent.html#why-does-it-work",
    "title": "Learning by gradient descent",
    "section": "Why does it work?",
    "text": "Why does it work?\nWhy does this method of stochastic gradient descent work so well, even for loss functions that are unimaginably complex? These are the answers I’ve been able to gather so far:\n\nMany loss functions in ML are designed to be convex (bowl-shaped).\nThere is redundancy in the data. If there are say 10 points all close together, the gradient calculated using just one of those points will be pretty close to the one calculated using all 10 points.\nWe don’t need to know the exact gradient, just an unbiased estimator of it. Put another way, if you want to get from San Francisco to LA, you don’t need the exact compass direction, you just need to get on one of the freeways going south."
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html#further-reading",
    "href": "posts/learning-by-gradient-descent.html#further-reading",
    "title": "Learning by gradient descent",
    "section": "Further Reading",
    "text": "Further Reading\n\n\n\n\n\n\n\nAndrew Ng, Lecture Notes for CS229, Spring 2022.\n\n\n\n\nThe structure of this post closely follows section 1.1 of these notes. Also note section 1.2 that contains the exact mathematical solution to the linear regression problem.\n\n\n\n\n\n\n\nMarc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong, Mathematics for Machine Learning, Chapter 7, 2020.\n\n\n\n\nSection 7.1 contains a detailed discussion of gradient descent methods, including more sophisticated ideas like momentum.\n\n\n\n\n\n\n\nLéon Bottou, et al., Optimization Methods for Large-Scale Machine Learning, 2016.\n\n\n\n\nSection 3.3 describes many motivations for using stochastic gradient descent and why it works so well."
  },
  {
    "objectID": "posts/learning-by-gradient-descent.html#notes-on-numpy",
    "href": "posts/learning-by-gradient-descent.html#notes-on-numpy",
    "title": "Learning by gradient descent",
    "section": "Notes on NumPy",
    "text": "Notes on NumPy\nNumPy supports vectorized versions of many operations. Two common cases of this are: (1) a scalar with an array, like 2 * X which multiplies each element of X by 2 (2) two arrays of the same shape, like Y - X which does an element-wise subtraction.\nIn the more general case NumPy also supports broadcasting where a smaller array is duplicated sufficiently to operate element-wise against a larger array."
  },
  {
    "objectID": "posts/breaking-caesar-cipher.html",
    "href": "posts/breaking-caesar-cipher.html",
    "title": "Breaking Caesar ciphers using relative entropy",
    "section": "",
    "text": "Last updated: Sep 1, 2024.\nOne of the simplest way to “encrypt” a piece of English text is the Caesar (rotational) cipher which shifts each letter by a fixed number of places in the alphabet with wrap around. A well-known example is rot13 where every letter is shifted by 13 places. So A -&gt; N, B -&gt; O and so on.\nI have often read that simple ciphers like this can be broken by “frequency analysis”, taking advantage of the observation that the frequencies of the various letters in English is well-known. But what exactly does that mean? How would such frequency analysis actually work?\nFirst, let’s write some code to encrypt and decrypt. For simplicity we’ll restrict our alphabet to just the uppercase letters A-Z and space and we’ll leave the space characters unchanged. The “key” for our encryption is a single number in the range [0, 25].\ndef rotate(c: str, n: int) -&gt; str:\n    \"\"\"\n    Rotate the character c by n places, wrapping around.\n    c must be an uppercase letter\n    \"\"\"\n\n    assert ord(\"A\") &lt;= ord(c) &lt;= ord(\"Z\") or c == \" \"\n    match c:\n        case \" \":\n            return c\n        case _:\n            return chr((ord(c) - ord(\"A\") + n) % 26 + ord(\"A\"))\n\n\ndef caesar_encrypt(s: str, n: int) -&gt; str:\n    return \"\".join([rotate(c, n) for c in s])\n\n\ndef caesar_decrypt(s: str, n: int) -&gt; str:\n    return \"\".join([rotate(c, -n) for c in s])\n\ncaesar_encrypt(\"THE MYSTIC CHORDS OF MEMORY\", 13)\n\n'GUR ZLFGVP PUBEQF BS ZRZBEL'\nOur task now is to figure out the key n when given only the encrypted text and the knowledge that a Caesar cipher has been used.\nSince we want to do frequency analysis in some fashion, let’s write a function to return the letter frequencies for a piece of text. For reasons that will become clear later we’ll actually compute a probability mass function, which is just the frequencies divided by the total count of letters. We will represent the PMF as a dictionary that maps each letter of our alphabet to its probability.\nfrom collections import Counter\n\nLetterPmf = dict[str, float]\n\ndef letter_pmf(s: str) -&gt; LetterPmf:\n    s = s.upper()\n    counts = Counter(s)\n    total = sum(counts.values())\n    return {c: counts[c] / total for c in counts.keys()}\n\nletter_pmf(\n    \"So we beat on boats against the current\" +\n    \"borne back ceaselessly into the past\"\n)\n\n{'S': 0.09333333333333334,\n 'O': 0.06666666666666667,\n ' ': 0.16,\n 'W': 0.013333333333333334,\n 'E': 0.12,\n 'B': 0.05333333333333334,\n 'A': 0.09333333333333334,\n 'T': 0.10666666666666667,\n 'N': 0.06666666666666667,\n 'G': 0.013333333333333334,\n 'I': 0.02666666666666667,\n 'H': 0.02666666666666667,\n 'C': 0.04,\n 'U': 0.013333333333333334,\n 'R': 0.04,\n 'K': 0.013333333333333334,\n 'L': 0.02666666666666667,\n 'Y': 0.013333333333333334,\n 'P': 0.013333333333333334}\nA reasonable way to use letter frequencies to break the cipher is:\n(Ofcourse we could argue that this is all needlessly complicated. If we already know that the text has been encrypted with a Caesar cipher, we can just inspect the results of trying all 26 keys and surely all but one of them will look like gibberish. That’s true, but in this post we’re more interested in teaching a computer to do that work for us.)\nLet’s use the complete Sherlock Holmes canon as a stand-in for the English language as a whole and compute the letter PMF.\ndef clean_text(s: str) -&gt; str:\n    return \"\".join([c for c in s if c.isalpha() or c == \" \"]).upper()\n\nHOLMES = letter_pmf(clean_text(open(\"data/holmes.txt\").read()))\nFor a moment let’s assume we have a way to measure how close two frequency distributions are and call it pmf_distance. The definition below is just a dummy, always returning 0.0.\ndef pmf_distance(p: LetterPmf, q: LetterPmf) -&gt; float:\n    return 0.0\nWe can now do the decryption:\nimport math\nimport numpy as np\n\n\ndef all_distances(cipher: str) -&gt; np.array:\n    \"\"\"\n    Return the PMF distances between the decrypted text and English (HOLMES)\n    for each of the keys in the range 0-25 (inclusive).\n    \"\"\"\n\n    return np.array([\n        pmf_distance(\n            letter_pmf(caesar_decrypt(cipher, key)),\n            HOLMES\n        )\n        for key in range(0, 26)\n    ])\n\n\ndef try_decrypt(cipher: str) -&gt; (int, str):\n    \"\"\"\n    Return the key and decrypted text, choosing the key that\n    yields the smallest PMF distance to English.\n    \"\"\"\n\n    correct_key = np.argmin(all_distances(cipher))\n    return correct_key, caesar_decrypt(cipher, correct_key)\n\nSECRET = (\n    \"ZV DL ILHA VU IVHAZ HNHPUZA AOL JBYYLUA IVYUL IHJR \" +\n    \"JLHZLSLZZSF PUAV AOL WHZA\"\n)\n\ntry_decrypt(SECRET)[1]\n\n'ZV DL ILHA VU IVHAZ HNHPUZA AOL JBYYLUA IVYUL IHJR JLHZLSLZZSF PUAV AOL WHZA'\nThis doesn’t get us anywhere ofcourse because we haven’t figured out what a good pmf_distance should be."
  },
  {
    "objectID": "posts/breaking-caesar-cipher.html#relative-entropy",
    "href": "posts/breaking-caesar-cipher.html#relative-entropy",
    "title": "Breaking Caesar ciphers using relative entropy",
    "section": "Relative entropy",
    "text": "Relative entropy\nThe Shannon information content of an event \\(x\\) with probability \\(P(x)\\) is defined as \\(log(1 / P(x))\\). This captures the intuition that the information obtainable from an event is proportional to how surprising it is. If you have a dog that barks at everything it’s not a very good guard dog because the bark tells you nothing and thus has low information content.\nThe entropy of a probability distribution is the sum of the information content of its events, weighted by the probability of each event.\n\\[\nH(x) = \\sum_{x_i} P(x_i) \\space log_2 \\frac{1}{P(x_i)}\n\\]\nThis too makes intuitive sense. For example, imagine a light that can be in two states on or off with equal probability. The entropy of this distribution is:\n\n0.5 * math.log2(1/0.5) + 0.5 * math.log2(1/0.5)\n\n1.0\n\n\nIn other words, the state of the light gives you 1 bit of information.\nThe relative entropy of a distribution \\(P(x)\\) compared to another distribution \\(Q(x)\\) captures some notion of the extra information we get from \\(P\\). Unfortunately the intuitive derivation of this is not at all clear to me so we’ll just have to trust the textbooks. The relative entropy is defined as:\n\\[\nD(P, Q) = \\sum_{x_i} P(x_i) \\space log_2 \\frac{P(x_i)}{Q(x_i)}\n\\]\n\ndef relative_entropy(p: LetterPmf, q: LetterPmf) -&gt; float:\n    return sum(p[x] * (math.log2(p[x]) - math.log2(q[x])) for x in p.keys())\n\nWe can also think of this as an informal kind of “distance” between \\(P\\) and \\(Q\\), while remembering that it’s not a true distance in the mathematical sense because it’s not symmetric (\\(D(P, Q) \\neq D(Q, P)\\) in general) and it doesn’t satisfy a few other distance-like properties. This distance is also known as the Kullback-Leibler divergence, a name that tells you nothing while being needlessly scary-looking.\nWe can try the decryption again, this time using the relative entropy as the distance between the distribution for each attempted decryption and the distribution for the English language:\n\npmf_distance = relative_entropy\ntry_decrypt(SECRET)[1]\n\n'SO WE BEAT ON BOATS AGAINST THE CURRENT BORNE BACK CEASELESSLY INTO THE PAST'\n\n\nWe can also visualize the relative entropy for each of the candidate keys to get a sense of how the frequency distribution with the correct key is so obviously different from that for all the wrong keys. We’ll plot the reciprocal of the distance just to make the graph prettier."
  },
  {
    "objectID": "posts/breaking-caesar-cipher.html#further-reading",
    "href": "posts/breaking-caesar-cipher.html#further-reading",
    "title": "Breaking Caesar ciphers using relative entropy",
    "section": "Further Reading",
    "text": "Further Reading\n\n\n\n\n\n\n\nDavid J. C. MacKay, Information Theory, Inference, and Learning Algorithms, 2003.\n\n\n\n\nSection 2.4 contains the definitions of entropy and relative entropy."
  }
]