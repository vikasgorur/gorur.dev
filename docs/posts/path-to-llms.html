<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Path to LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-07ba0ad10f5680c660e360ac31d2f3b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c6f10c52b16997cd7acf54b121968426.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Path to LLMs</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>Last updated</em>: Oct 18, 2024.</p>
<hr>
<p>This post is my attempt to draw the shortest path from knowing a little bit of ML to understanding state of the art language models. It includes both milestone papers and the best resources I’ve found to understand a concept. I also like knowing the history of things so there will be a bunch of papers that might really only be of historical interest.</p>
<p>This is a <em>personal</em> path, with the goal of being a reasonably good practitioner of ML, not a researcher. Finally, “path” is a misnomer. It’s more like a garden to get lost in.</p>
<section id="math-background" class="level2">
<h2 class="anchored" data-anchor-id="math-background">Math background</h2>
<p>There is no end to the amount of math one <em>could</em> learn before studying ML, and usually the more I learn the more it seems to help. However, I’ve also found that it’s ok to “lazy-load” the required math once you’ve acquired a decent intuition in each of the major areas. This section therefore is just going to be a list of the areas of math that can be helpful and the best resources I’ve found for learning them.</p>
<p>Ever since I discovered computers my identity has been “programmer”. The book by <span class="citation" data-cites="jeremykunProgrammersIntroductionMathematics2021">Jeremy Kun (<a href="#ref-jeremykunProgrammersIntroductionMathematics2021" role="doc-biblioref">2021</a>)</span> changed my <em>relationship</em> to math and gave me the confidence to read the ML textbooks and papers. It helped me reconnect with my teenage self that found math playful and was excited by it rather than scared by notation. <strong>This is a life-changing book.</strong></p>
<section id="probability" class="level3">
<h3 class="anchored" data-anchor-id="probability">Probability</h3>
<p>Probability is the foundation for all of ML, statistics, and science. It’s also <em>way more</em> complicated than our brief encounter with it in high school or college makes us believe. I’m always on the look out for books and articles that help in developing a good intuition for probability.</p>
<p>The textbook by <span class="citation" data-cites="hammingArtProbabilityScientists1991">(<a href="#ref-hammingArtProbabilityScientists1991" role="doc-biblioref">Hamming 1991</a>)</span> is one of the best introductions. It is rigorous enough for us engineers but more importantly has long passages that explain the intuition behind ideas.</p>
</section>
<section id="information-theory" class="level3">
<h3 class="anchored" data-anchor-id="information-theory">Information Theory</h3>
<p>Information seems like the most natural concept to try to understand ML and stats. Many of the questions of interest can be posed as information theory questions: “what has a model learnt?”, or “what did this experiment tell us?”, “how much can a model of a certain size learn?”</p>
<p><span class="citation" data-cites="coverElementsInformationTheory2005">(<a href="#ref-coverElementsInformationTheory2005" role="doc-biblioref">Cover and Thomas 2005</a>)</span> and <span class="citation" data-cites="mackayInformationTheoryInference2003">(<a href="#ref-mackayInformationTheoryInference2003" role="doc-biblioref">MacKay 2003</a>)</span> are two useful textbooks.</p>
</section>
<section id="linear-algebra" class="level3">
<h3 class="anchored" data-anchor-id="linear-algebra">Linear Algebra</h3>
<p>Linear Algebra has the worst branding in all of math. It’s more exciting to think of the subject as “thinking in high-dimensional spaces”. Everything in ML deals with vectors with impossibly high dimensions (for example, each token in GPT3 is represented as a vector in a ~50,000 dimension space).</p>
<p>The video series “Essence of Linear Algebra” by <span class="citation" data-cites="3blue1brownEssenceLinearAlgebra2016">(<a href="#ref-3blue1brownEssenceLinearAlgebra2016" role="doc-biblioref">3blue1brown 2016</a>)</span> was the first time linear algebra made any intuitive sense to me.</p>
</section>
<section id="calculus" class="level3">
<h3 class="anchored" data-anchor-id="calculus">Calculus</h3>
<p>ML papers are full of complicated equations with symbols from multivariate and matrix calculus. This might give the impression that one needs a full undergrad course in these topics before making any progress, but I don’t buy it. I think one can get by for a long time with just the intuition of the concept of a derivative (gradient) for complicated functions and the chain rule for computing them.</p>
</section>
</section>
<section id="optimization-in-ml" class="level2">
<h2 class="anchored" data-anchor-id="optimization-in-ml">Optimization in ML</h2>
<p>The goal of all ML training is to find an acceptably low value of the loss function. This is the part of ML that I find it the easiest to treat as a black box.</p>
<p><span class="citation" data-cites="bottouOptimizationMethodsLargeScale2016">(<a href="#ref-bottouOptimizationMethodsLargeScale2016" role="doc-biblioref">Bottou, Curtis, and Nocedal 2016</a>)</span> is a great overview of the various optimization methods used in ML.</p>
</section>
<section id="automatic-differentiation" class="level2">
<h2 class="anchored" data-anchor-id="automatic-differentiation">Automatic Differentiation</h2>
<p>AD is the key to training large neural networks. AD libraries automatically figure out the gradient of the loss function as long as the computation of the loss function is expressed in a form that the library expects. For example, in PyTorch the computation is expressed as tensor operations.</p>
<p><span class="citation" data-cites="baydinAutomaticDifferentiationMachine2015">(<a href="#ref-baydinAutomaticDifferentiationMachine2015" role="doc-biblioref">Baydin et al. 2015</a>)</span> is a great survey of the various AD methods. For ML training we care about “reverse mode”. <span class="citation" data-cites="paszkePyTorchImperativeStyle2019">(<a href="#ref-paszkePyTorchImperativeStyle2019" role="doc-biblioref">Paszke et al. 2019</a>)</span> describes PyTorch, the most widely used library for deep learning in production.</p>
</section>
<section id="what-are-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="what-are-neural-networks">What are neural networks?</h2>
<p>The first neural network was the perceptron <span class="citation" data-cites="nilssonQuestArtificialIntelligence2010">(see <a href="#ref-nilssonQuestArtificialIntelligence2010" role="doc-biblioref">Nilsson 2010, sec. 4.2.1</a>)</span>, a single-layer network built to identify objects in 20x20 pixel images. I find it fascinating to note that most of the early work on neural networks was done by people trying to understand human cognition by building a model of computation different from the familiar digital (von Neumann) computer. From that perspective, current LLMs running on GPUs are just one physical realization of the model of computation.</p>
<p>The key algorithm for training neural networks is backpropagation. This algorithm has apparently been invented independently many times. <span class="citation" data-cites="rumelhartLearningRepresentationsBackpropagating1986">(<a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span> is one of the widely cited descriptions of it.</p>
<p><span class="citation" data-cites="lecunBackpropagationAppliedHandwritten1989">(<a href="#ref-lecunBackpropagationAppliedHandwritten1989" role="doc-biblioref">LeCun et al. 1989</a>)</span> is one of the first examples of using neural networks and back propagation to solve the recognizably modern problem of handwriting recognition. An interesting companion piece is the blog post <span class="citation" data-cites="karpathyDeepNeuralNets2022">(<a href="#ref-karpathyDeepNeuralNets2022" role="doc-biblioref">Karpathy 2022a</a>)</span> that re-implements the network described in the original paper and illustrates the massive difference in training time made possible by modern hardware.</p>
<p>Another milestone in the deep learning revolution is AlexNet <span class="citation" data-cites="krizhevskyImageNetClassificationDeep2012">(<a href="#ref-krizhevskyImageNetClassificationDeep2012" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2012</a>)</span> where a deep learning model beat all other previous computer vision models on image recognition by a significant margin. This paper also illustrates the coming together of three factors that make deep learning practical and are true to this day: (1) massive datasets (2) GPUs for efficient matrix computations (3) libraries to do automatic differentiation easily.</p>
</section>
<section id="what-is-language-modeling" class="level2">
<h2 class="anchored" data-anchor-id="what-is-language-modeling">What is language modeling?</h2>
<p>The task of language modeling is to learn a probability distribution about a corpus. The distribution is the conditional probability of the next token given a sequence of previous tokens. A short introduction to language modeling is in <span class="citation" data-cites="hangliLanguageModelsPresent2022">(<a href="#ref-hangliLanguageModelsPresent2022" role="doc-biblioref">Hang Li 2022</a>)</span>.</p>
<p>The roots of this go back to Markov analyzing Pushkin’s poetry to settle a debate about free will(!), described in the article by <span class="citation" data-cites="hayesFirstLinksMarkov2013">(<a href="#ref-hayesFirstLinksMarkov2013" role="doc-biblioref">Hayes 2013</a>)</span>.</p>
<p>The classic <span class="citation" data-cites="shannonMathematicalTheoryCommunication1948">(<a href="#ref-shannonMathematicalTheoryCommunication1948" role="doc-biblioref">Shannon 1948</a>)</span> paper that invented information theory also considers language modeling, as does his subsequent paper <span class="citation" data-cites="shannonPredictionEntropyPrinted1951">(<a href="#ref-shannonPredictionEntropyPrinted1951" role="doc-biblioref">Shannon 1951</a>)</span>. In the second paper he describes an experiment to figure out the entropy of English language by giving humans (his wife and another couple) the task of predicting the next word of a short sentence, essentially treating them like modern LLMs!</p>
<p>The Shannon living-room experiments story is related in this entertaining profile: <span class="citation" data-cites="horganClaudeShannonProfile1992">(<a href="#ref-horganClaudeShannonProfile1992" role="doc-biblioref">Horgan 1992</a>)</span></p>
</section>
<section id="how-is-language-modeling-done-with-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="how-is-language-modeling-done-with-neural-networks">How is language modeling done with neural networks?</h2>
<p><span class="citation" data-cites="bengioNeuralProbabilisticLanguage2003">(<a href="#ref-bengioNeuralProbabilisticLanguage2003" role="doc-biblioref">Bengio et al. 2003</a>)</span> introduced the ideas of using a neural network to model language as well as the idea of a “distributed representation”, also known as word embeddings. The goal of embedding is to turn words and phrases into vectors in a high-dimensional space.</p>
<p>A big step forward in embeddings was Google’s word2vec paper <span class="citation" data-cites="mikolovDistributedRepresentationsWords2013">(<a href="#ref-mikolovDistributedRepresentationsWords2013" role="doc-biblioref">Mikolov et al. 2013</a>)</span>, which contained the famous example <code>vec("King") - vec("Man") + vec("Woman") ~= vec("Queen")</code>. Embeddings just on their own are an incredibly useful tool in building products because they capture a general notion of semantic “distance” between words, sentences, or entire documents.</p>
<p>Recurrent Neural Nets (RNNs) were one solution to the problem of capturing the <em>sequential</em> nature of language. The historical roots of this approach are in the cognitive science paper <span class="citation" data-cites="elmanFindingStructureTime1990">(<a href="#ref-elmanFindingStructureTime1990" role="doc-biblioref">Elman 1990</a>)</span>. The blog post <span class="citation" data-cites="andrejkarpathyUnreasonableEffectivenessRecurrent2015">(<a href="#ref-andrejkarpathyUnreasonableEffectivenessRecurrent2015" role="doc-biblioref">Andrej Karpathy 2015</a>)</span> illustrates the “unreasonable effectiveness” of RNNs.</p>
<p>The playlist “Neural Networks: Zero to Hero” <span class="citation" data-cites="karpathyNeuralNetworksZero2022">(<a href="#ref-karpathyNeuralNetworksZero2022" role="doc-biblioref">Karpathy 2022b</a>)</span> is a step-by-step walkthrough to building something like GPT-2 starting from nothing but knowledge of Python. This entire post is in a sense is all the supplementary reading I’m doing to finish understanding all the videos in this playlist.</p>
</section>
<section id="large-language-models" class="level2">
<h2 class="anchored" data-anchor-id="large-language-models">Large Language Models</h2>
<p>Everything in this section is just the starting point for deeper rabbit holes.</p>
<p>“Attention is all you need” <span class="citation" data-cites="vaswaniAttentionAllYou2017">(<a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span> contains the core DNA of all current LLMs. Everything I described in this post above is my attempt to get to a full understanding of this landmark paper.</p>
<p>“State of GPT” <span class="citation" data-cites="andrejkarpathyStateGPT2023">(<a href="#ref-andrejkarpathyStateGPT2023" role="doc-biblioref">Andrej Karpathy 2023</a>)</span> is the best 1-hour introduction to the architecture, training and capabilities of LLMs. This talk is accessible to any working programmer, it doesn’t need any previous knowledge of LLMs or neural networks.</p>
<p><span class="citation" data-cites="3blue1brownNeuralNetworks2024">(<a href="#ref-3blue1brownNeuralNetworks2024" role="doc-biblioref">3blue1brown 2024</a>)</span> is a great series of videos on neural networks and deep learning, with recent videos focusing on LLMs.</p>
<p>The papers on open source LLMs have a wealth of detail on the training data and methodology. See LLAMA2 <span class="citation" data-cites="touvronLlama2Open2023">(<a href="#ref-touvronLlama2Open2023" role="doc-biblioref">Touvron et al. 2023</a>)</span>, Mistral 7B <span class="citation" data-cites="jiangMistral7B2023">(<a href="#ref-jiangMistral7B2023" role="doc-biblioref">Jiang et al. 2023</a>)</span>.</p>
<p>The effectiveness of neural networks is extremely dependent on the quantity and quality of the training data. This fact is apparently discovered again and again so often that it has a name: “the bitter lesson” <span class="citation" data-cites="richsuttonBitterLesson2019">(<a href="#ref-richsuttonBitterLesson2019" role="doc-biblioref">Rich Sutton 2019</a>)</span>. An intriguing related fact about LLMs is the existence of “scaling laws” that describe the optimal model size and number of training tokens for a given compute budget <span class="citation" data-cites="hoffmannTrainingComputeOptimalLarge2022">(<a href="#ref-hoffmannTrainingComputeOptimalLarge2022" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>.</p>
<p>The wide applicability of LLMs is a result of their ability to learn to perform tasks with just a handful of examples (“few-shot learning”). This discovery is related in the GPT2 <span class="citation" data-cites="brownLanguageModelsAre2020">(<a href="#ref-brownLanguageModelsAre2020" role="doc-biblioref">Brown et al. 2020</a>)</span> and GPT3 papers <span class="citation" data-cites="kojimaLargeLanguageModels2022">(<a href="#ref-kojimaLargeLanguageModels2022" role="doc-biblioref">Kojima et al. 2022</a>)</span>.</p>
<p>Training LLMs is an incredibly complicated systems engineering problem. This blog post by the lead of PyTorch <span class="citation" data-cites="chintalaHowTrainModel2024">(<a href="#ref-chintalaHowTrainModel2024" role="doc-biblioref">Chintala 2024</a>)</span> and the infrastructure section in the LLama3 paper <span class="citation" data-cites="dubeyLlama3Herd2024">(<a href="#ref-dubeyLlama3Herd2024" role="doc-biblioref">Dubey et al. 2024</a>)</span> provide insight into what it takes.</p>
<p>[ <em>to be continued</em> … ]</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-3blue1brownEssenceLinearAlgebra2016" class="csl-entry" role="listitem">
3blue1brown. 2016. <span>“Essence of <span>Linear Algebra</span>.”</span> <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab</a>.
</div>
<div id="ref-3blue1brownNeuralNetworks2024" class="csl-entry" role="listitem">
———. 2024. <span>“Neural <span>Networks</span>.”</span> <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a>.
</div>
<div id="ref-andrejkarpathyUnreasonableEffectivenessRecurrent2015" class="csl-entry" role="listitem">
Andrej Karpathy. 2015. <span>“The <span>Unreasonable Effectiveness</span> of <span>Recurrent Neural Networks</span>.”</span> <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">https://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>.
</div>
<div id="ref-andrejkarpathyStateGPT2023" class="csl-entry" role="listitem">
———. 2023. <span>“State of <span>GPT</span>.”</span> <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">https://www.youtube.com/watch?v=bZQun8Y4L2A</a>.
</div>
<div id="ref-baydinAutomaticDifferentiationMachine2015" class="csl-entry" role="listitem">
Baydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2015. <span>“Automatic Differentiation in Machine Learning: A Survey.”</span> <a href="https://doi.org/10.48550/ARXIV.1502.05767">https://doi.org/10.48550/ARXIV.1502.05767</a>.
</div>
<div id="ref-bengioNeuralProbabilisticLanguage2003" class="csl-entry" role="listitem">
Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. <span>“A Neural Probabilistic Language Model.”</span> <em>J. Mach. Learn. Res.</em> 3 (null): 1137–55.
</div>
<div id="ref-bottouOptimizationMethodsLargeScale2016" class="csl-entry" role="listitem">
Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. 2016. <span>“Optimization <span>Methods</span> for <span>Large-Scale Machine Learning</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1606.04838">https://doi.org/10.48550/ARXIV.1606.04838</a>.
</div>
<div id="ref-brownLanguageModelsAre2020" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language <span>Models</span> Are <span>Few-Shot Learners</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2005.14165">https://doi.org/10.48550/ARXIV.2005.14165</a>.
</div>
<div id="ref-chintalaHowTrainModel2024" class="csl-entry" role="listitem">
Chintala, Soumith. 2024. <span>“How to Train a Model on 10k <span>H100 GPUs</span>?”</span> <a href="https://soumith.ch/blog/2024-10-02-training-10k-scale.md.html">https://soumith.ch/blog/2024-10-02-training-10k-scale.md.html</a>.
</div>
<div id="ref-coverElementsInformationTheory2005" class="csl-entry" role="listitem">
Cover, Thomas M., and Joy A. Thomas. 2005. <em>Elements of <span>Information Theory</span></em>. 1st ed. Wiley. <a href="https://doi.org/10.1002/047174882X">https://doi.org/10.1002/047174882X</a>.
</div>
<div id="ref-dubeyLlama3Herd2024" class="csl-entry" role="listitem">
Dubey, Abhimanyu, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, et al. 2024. <span>“The <span>Llama</span> 3 <span>Herd</span> of <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2407.21783">https://doi.org/10.48550/ARXIV.2407.21783</a>.
</div>
<div id="ref-elmanFindingStructureTime1990" class="csl-entry" role="listitem">
Elman, Jeffrey L. 1990. <span>“Finding <span>Structure</span> in <span>Time</span>.”</span> <em>Cognitive Science</em> 14 (2): 179–211. <a href="https://doi.org/10.1207/s15516709cog1402_1">https://doi.org/10.1207/s15516709cog1402_1</a>.
</div>
<div id="ref-hammingArtProbabilityScientists1991" class="csl-entry" role="listitem">
Hamming, Richard W. 1991. <em>The <span>Art</span> of <span>Probability</span>: <span>For Scientists</span> and <span>Engineers</span></em>. 1st ed. CRC Press. <a href="https://doi.org/10.1201/9780429492952">https://doi.org/10.1201/9780429492952</a>.
</div>
<div id="ref-hangliLanguageModelsPresent2022" class="csl-entry" role="listitem">
Hang Li. 2022. <span>“Language Models: Past, Present, and Future.”</span> <em>Communications of the ACM</em> 65 (7): 56–63. <a href="https://doi.org/10.1145/3490443">https://doi.org/10.1145/3490443</a>.
</div>
<div id="ref-hayesFirstLinksMarkov2013" class="csl-entry" role="listitem">
Hayes, Brian. 2013. <span>“First <span>Links</span> in the <span>Markov Chain</span>.”</span> <em>American Scientist</em> 101 (2). <a href="https://doi.org/10.1511/2013.101.92">https://doi.org/10.1511/2013.101.92</a>.
</div>
<div id="ref-hoffmannTrainingComputeOptimalLarge2022" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>“Training <span>Compute-Optimal Large Language Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2203.15556">https://doi.org/10.48550/ARXIV.2203.15556</a>.
</div>
<div id="ref-horganClaudeShannonProfile1992" class="csl-entry" role="listitem">
Horgan, J. 1992. <span>“Claude <span>E</span>. <span>Shannon</span> [<span>Profile</span>].”</span> <em>IEEE Spectrum</em> 29 (4): 72–75. <a href="https://doi.org/10.1109/MSPEC.1992.672257">https://doi.org/10.1109/MSPEC.1992.672257</a>.
</div>
<div id="ref-jeremykunProgrammersIntroductionMathematics2021" class="csl-entry" role="listitem">
Jeremy Kun. 2021. <em>A <span>Programmer</span>’s <span>Introduction</span> to <span>Mathematics</span></em>. <a href="https://pimbook.org/">https://pimbook.org/</a>.
</div>
<div id="ref-jiangMistral7B2023" class="csl-entry" role="listitem">
Jiang, Albert Q., Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, et al. 2023. <span>“Mistral <span>7B</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2310.06825">https://doi.org/10.48550/ARXIV.2310.06825</a>.
</div>
<div id="ref-karpathyDeepNeuralNets2022" class="csl-entry" role="listitem">
Karpathy, Andrej. 2022a. <span>“Deep <span>Neural Nets</span>: 33 Years Ago and 33 Years from Now.”</span> <a href="https://karpathy.github.io/2022/03/14/lecun1989/">https://karpathy.github.io/2022/03/14/lecun1989/</a>.
</div>
<div id="ref-karpathyNeuralNetworksZero2022" class="csl-entry" role="listitem">
———. 2022b. <span>“Neural <span>Networks</span>: <span>Zero</span> to <span>Hero</span>.”</span> <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ</a>.
</div>
<div id="ref-kojimaLargeLanguageModels2022" class="csl-entry" role="listitem">
Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. <span>“Large <span>Language Models</span> Are <span>Zero-Shot Reasoners</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2205.11916">https://doi.org/10.48550/ARXIV.2205.11916</a>.
</div>
<div id="ref-krizhevskyImageNetClassificationDeep2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“<span>ImageNet Classification</span> with <span>Deep Convolutional Neural Networks</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.
</div>
<div id="ref-lecunBackpropagationAppliedHandwritten1989" class="csl-entry" role="listitem">
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. <span>“Backpropagation <span>Applied</span> to <span>Handwritten Zip Code Recognition</span>.”</span> <em>Neural Computation</em> 1 (4): 541–51. <a href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a>.
</div>
<div id="ref-mackayInformationTheoryInference2003" class="csl-entry" role="listitem">
MacKay, David J. C. 2003. <em>Information Theory, Inference, and Learning Algorithms</em>. 22nd printing. Cambridge: Cambridge University Press.
</div>
<div id="ref-mikolovDistributedRepresentationsWords2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Distributed <span>Representations</span> of <span>Words</span> and <span>Phrases</span> and Their <span>Compositionality</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1310.4546">https://doi.org/10.48550/ARXIV.1310.4546</a>.
</div>
<div id="ref-nilssonQuestArtificialIntelligence2010" class="csl-entry" role="listitem">
Nilsson, Nils J. 2010. <em>The Quest for Artificial Intelligence: A History of Ideas and Achievements</em>. Cambridge ; New York: Cambridge University Press.
</div>
<div id="ref-paszkePyTorchImperativeStyle2019" class="csl-entry" role="listitem">
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. <span>“<span>PyTorch</span>: <span>An Imperative Style</span>, <span>High-Performance Deep Learning Library</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1912.01703">https://doi.org/10.48550/ARXIV.1912.01703</a>.
</div>
<div id="ref-richsuttonBitterLesson2019" class="csl-entry" role="listitem">
Rich Sutton. 2019. <span>“The <span>Bitter Lesson</span>.”</span> <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>.
</div>
<div id="ref-rumelhartLearningRepresentationsBackpropagating1986" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div>
<div id="ref-shannonMathematicalTheoryCommunication1948" class="csl-entry" role="listitem">
Shannon, Claude. 1948. <span>“A <span>Mathematical Theory</span> of <span>Communication</span>.”</span> <em>Bell System Technical Journal</em> 27. <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</a>.
</div>
<div id="ref-shannonPredictionEntropyPrinted1951" class="csl-entry" role="listitem">
———. 1951. <span>“Prediction and <span>Entropy</span> of <span>Printed English</span>.”</span> <em>Bell System Technical Journal</em> 30 (1): 50–64. <a href="https://doi.org/10.1002/j.1538-7305.1951.tb01366.x">https://doi.org/10.1002/j.1538-7305.1951.tb01366.x</a>.
</div>
<div id="ref-touvronLlama2Open2023" class="csl-entry" role="listitem">
Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. <span>“Llama 2: <span>Open Foundation</span> and <span>Fine-Tuned Chat Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2307.09288">https://doi.org/10.48550/ARXIV.2307.09288</a>.
</div>
<div id="ref-vaswaniAttentionAllYou2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention <span>Is All You Need</span>.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1706.03762">https://doi.org/10.48550/ARXIV.1706.03762</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>