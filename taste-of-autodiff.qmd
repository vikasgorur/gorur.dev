---
title: "A Taste of Automatic Differentiation"
jupyter: julia-1.9
format:
    html:
        theme: flatly
        mainfont: "PT Serif"
        code-fold: false
---

Large language models (LLMs) are magic. I want to answer the question, "where is the magic?"
by writing simple, readable code to illustrate all the key ideas that make the magic happen.
The code is written in Julia but I try not to use any esoteric features so that it can be understood
by programmers coming from any language.

This is the first post in the series, focusing on Automatic Differentiation (AD). The idea of AD
starts with the fascinating question: what if we could differentiate *code*? What problems
would that allow us to solve?

We will try out that exercise with a couple of toy examples and build up to understanding its relevance to training neural networks.

We start by recalling the definition of a derivative:

$$
f'(x) = \lim_{\delta x \to 0} \frac{f(x + \delta x) - f(x)}{\delta x}
$$

For example,

$$
f(x) = x^2
$$
$$
f'(x) = 2x
$$

The intuition of the derivative of $f$ at a point $x_0$ is that it measures how *sensitive* the output is to a small change in the input around the neighborhood of $x_0$.

We can translate the definition into code in a straight-forward way:

```{julia}
function d(f, x)
    δ = 1e-6
    return (f(x + δ) - f(x)) / δ
end

f(x) = x^2
print(d(f, 2.0))
```

The code above is an example of *numeric differentiation*. While this approach isn't efficient or suitable
for ML applications, it's sufficient for our first toy example.

## Example 1: Linear Regression

The problem of linear regression is to fit the "best" line through a set of points. For
our example we'll use a dataset with just one input variable `n` and one output variable `duration`.

This is a real-world dataset I gathered by measuring the time it takes to train a particular
ML model as a function of the number of training examples. While the time it takes for a training job
to run might seem like the result of a complicated process, the relationship with the number of training
examples is almost perfectly linear. It's a nice confirmation for me that linear relationships arise
naturally in the real world and are not just found in statistics textbooks.

```{julia}
import CSV
using DataFrames

metrics = CSV.read("metrics.csv", DataFrame)
metrics[:, [:n, :duration]]
```

We'll think of the input and output variables as the vectors $\textbf{X}$ and $\textbf{Y}$. We will also scale
the raw data so that each of these vectors contains values only in the range $[0, 1]$. Our aim
is to find the best parameters ($\beta_1$, $\beta_2$) for the line:
$$
\textbf{Y} = \beta_1 \textbf{X} + \beta_2
$$

```{julia}
scale(V) = (V .- minimum(V)) ./ (maximum(V) - minimum(V))

X = scale(metrics[!, :n])
Y = scale(metrics[!, :duration])

```

To actually find the parameters we need to define the notion of the "best fit". We can think of
the line we're trying to fit as a prediction model. We can then measure its fit by computing
the difference between the correct answer and the predicted answer for each data point and
adding up the differences. We will also square each of the differences so that it doesn't matter
if the prediction is off in either direction, only the magnitude is relevant.

(TODO: why not just use the absolute value? Is it because historically it was easier to differentiate the squared
loss?)

Loss function
gradient descent

visualize the gradient

## Further reading

Much deeper blog post https://jingnanshi.com/blog/autodiff.html

Karpathy video.
Autodiff survey paper.
PyTorch paper.
Zygote.jl

https://thenumb.at/Autodiff/

https://www.assemblyai.com/blog/differentiable-programming-a-simple-introduction/

zygote paper https://arxiv.org/abs/1810.07951

perfect hashing https://www.cs.cmu.edu/~avrim/451/lectures/lect0916.pdf

Given a set of company names, find a simple similarity function
that can tolerate upto a certain amount of typos.

## Julia footnotes

Explain broadcasting.

Outline

concept of gradient

The four kinds of calculating the gradient:
- symbolic
- numeric (prone to rounding instability)

"auto" depends on maintaining "evaluation traces" (= Wengert list = expression DAG)

with the DAG, we can propagate the gradient forwards or backwards.

forwards requires 1 pass for each variable
backwards can compute all

using that to solve linear regression with descent

gradient with multiple params - polynomial interpolation
reverse-mode
Zygote - auto gradient (ref paper)

using better optimization (BFGS)

PyTorch & relevance to neural networks.

Forward-mode AD in Julia https://arxiv.org/pdf/1607.07892.pdf

Eng tradeoffs in various libraries http://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/

